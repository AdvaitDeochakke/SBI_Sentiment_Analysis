{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import pandas as pd\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import time\n",
    "from nltk.stem import SnowballStemmer\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import zipfile\n",
    "# keras modeling. \n",
    "# i love the simplicity, but not having 12.1 cuda support on my system kinda hurts\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
    "from keras import utils\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # GPU/CUDA is available\n",
    "    print(\"GPU is available\")\n",
    "    # You can also print the details of the available GPUs\n",
    "    for gpu in gpus:\n",
    "        print(\"GPU Name:\", gpu.name)\n",
    "else:\n",
    "    # No GPU/CUDA available\n",
    "    print(\"GPU is not available\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dont have tensorflow/keras compatibility until i downgrade from cuda 12.1\n",
    "\n",
    "cant be bothered tbh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to generate progress reports for our stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative    748869\n",
       "Positive    734073\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('preprocessed_dataset.csv')\n",
    "df.target.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(n=160000)\n",
    "# df.target.value_counts()\n",
    "\n",
    "# explanation : was trying on smaller dataset to just get a quick idea\n",
    "# whether stuff works or not\n",
    "# now i know it works, so no need to limit myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target    0\n",
      "Clean     1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n",
    "df.dropna(inplace=True) # clean na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative    524643\n",
      "Positive    513415\n",
      "Name: target, dtype: int64\n",
      "Negative    224225\n",
      "Positive    220658\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# simple train test split\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.3, random_state=42)\n",
    "print(train_data.target.value_counts())\n",
    "print(test_data.target.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1199674</th>\n",
       "      <td>Positive</td>\n",
       "      <td>beethoven western x49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855734</th>\n",
       "      <td>Positive</td>\n",
       "      <td>final sleep silenc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517948</th>\n",
       "      <td>Negative</td>\n",
       "      <td>sorri class call what plan get togeth everyon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648172</th>\n",
       "      <td>Negative</td>\n",
       "      <td>dad btard mom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801393</th>\n",
       "      <td>Positive</td>\n",
       "      <td>hockey kill tonigh saw gab</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           target                                              Clean\n",
       "1199674  Positive                              beethoven western x49\n",
       "855734   Positive                                 final sleep silenc\n",
       "517948   Negative  sorri class call what plan get togeth everyon ...\n",
       "648172   Negative                                      dad btard mom\n",
       "801393   Positive                         hockey kill tonigh saw gab"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we start the phrase generation  \n",
    "saves time on modeling  \n",
    "a simple bigram phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['upset',\n",
       "  'cant',\n",
       "  'updat',\n",
       "  'facebook',\n",
       "  'text',\n",
       "  'might',\n",
       "  'cri',\n",
       "  'result',\n",
       "  'school',\n",
       "  'today',\n",
       "  'also',\n",
       "  'blah'],\n",
       " ['dive',\n",
       "  'mani',\n",
       "  'time',\n",
       "  'ball',\n",
       "  'manag',\n",
       "  'save',\n",
       "  '50',\n",
       "  'rest',\n",
       "  'go',\n",
       "  'bound'],\n",
       " ['whole', 'bodi', 'feel', 'itchi', 'like', 'fire'],\n",
       " ['behav', 'im', 'mad', 'cant', 'see'],\n",
       " ['whole', 'crew']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "unigrams = [_.split() for _ in df.Clean]\n",
    "unigrams[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 18:10:50,747 : INFO : collecting all words and their counts\n",
      "2023-06-27 18:10:50,748 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2023-06-27 18:10:50,866 : INFO : PROGRESS: at sentence #10000, processed 76095 words and 65490 word types\n",
      "2023-06-27 18:10:51,006 : INFO : PROGRESS: at sentence #20000, processed 152132 words and 118301 word types\n",
      "2023-06-27 18:10:51,129 : INFO : PROGRESS: at sentence #30000, processed 228323 words and 166189 word types\n",
      "2023-06-27 18:10:51,243 : INFO : PROGRESS: at sentence #40000, processed 305846 words and 212760 word types\n",
      "2023-06-27 18:10:51,358 : INFO : PROGRESS: at sentence #50000, processed 384509 words and 259058 word types\n",
      "2023-06-27 18:10:51,456 : INFO : PROGRESS: at sentence #60000, processed 461796 words and 302501 word types\n",
      "2023-06-27 18:10:51,565 : INFO : PROGRESS: at sentence #70000, processed 539336 words and 343479 word types\n",
      "2023-06-27 18:10:51,682 : INFO : PROGRESS: at sentence #80000, processed 616824 words and 383817 word types\n",
      "2023-06-27 18:10:51,794 : INFO : PROGRESS: at sentence #90000, processed 696138 words and 423877 word types\n",
      "2023-06-27 18:10:51,900 : INFO : PROGRESS: at sentence #100000, processed 774774 words and 463948 word types\n",
      "2023-06-27 18:10:52,013 : INFO : PROGRESS: at sentence #110000, processed 853315 words and 502809 word types\n",
      "2023-06-27 18:10:52,127 : INFO : PROGRESS: at sentence #120000, processed 931387 words and 540691 word types\n",
      "2023-06-27 18:10:52,239 : INFO : PROGRESS: at sentence #130000, processed 1009855 words and 578095 word types\n",
      "2023-06-27 18:10:52,341 : INFO : PROGRESS: at sentence #140000, processed 1090201 words and 616448 word types\n",
      "2023-06-27 18:10:52,445 : INFO : PROGRESS: at sentence #150000, processed 1168984 words and 653404 word types\n",
      "2023-06-27 18:10:52,558 : INFO : PROGRESS: at sentence #160000, processed 1248134 words and 689949 word types\n",
      "2023-06-27 18:10:52,697 : INFO : PROGRESS: at sentence #170000, processed 1326420 words and 725596 word types\n",
      "2023-06-27 18:10:52,805 : INFO : PROGRESS: at sentence #180000, processed 1404278 words and 759626 word types\n",
      "2023-06-27 18:10:52,919 : INFO : PROGRESS: at sentence #190000, processed 1483968 words and 794749 word types\n",
      "2023-06-27 18:10:53,023 : INFO : PROGRESS: at sentence #200000, processed 1562965 words and 829208 word types\n",
      "2023-06-27 18:10:53,141 : INFO : PROGRESS: at sentence #210000, processed 1640593 words and 862892 word types\n",
      "2023-06-27 18:10:53,250 : INFO : PROGRESS: at sentence #220000, processed 1718129 words and 895518 word types\n",
      "2023-06-27 18:10:53,377 : INFO : PROGRESS: at sentence #230000, processed 1797379 words and 928857 word types\n",
      "2023-06-27 18:10:53,489 : INFO : PROGRESS: at sentence #240000, processed 1876079 words and 961192 word types\n",
      "2023-06-27 18:10:53,598 : INFO : PROGRESS: at sentence #250000, processed 1954098 words and 992353 word types\n",
      "2023-06-27 18:10:53,718 : INFO : PROGRESS: at sentence #260000, processed 2034418 words and 1025215 word types\n",
      "2023-06-27 18:10:53,831 : INFO : PROGRESS: at sentence #270000, processed 2113818 words and 1057428 word types\n",
      "2023-06-27 18:10:53,954 : INFO : PROGRESS: at sentence #280000, processed 2192372 words and 1088960 word types\n",
      "2023-06-27 18:10:54,067 : INFO : PROGRESS: at sentence #290000, processed 2270374 words and 1119618 word types\n",
      "2023-06-27 18:10:54,180 : INFO : PROGRESS: at sentence #300000, processed 2350142 words and 1151218 word types\n",
      "2023-06-27 18:10:54,296 : INFO : PROGRESS: at sentence #310000, processed 2429724 words and 1183347 word types\n",
      "2023-06-27 18:10:54,414 : INFO : PROGRESS: at sentence #320000, processed 2508692 words and 1214036 word types\n",
      "2023-06-27 18:10:54,528 : INFO : PROGRESS: at sentence #330000, processed 2589057 words and 1246174 word types\n",
      "2023-06-27 18:10:54,633 : INFO : PROGRESS: at sentence #340000, processed 2668462 words and 1277082 word types\n",
      "2023-06-27 18:10:54,746 : INFO : PROGRESS: at sentence #350000, processed 2747227 words and 1306966 word types\n",
      "2023-06-27 18:10:54,862 : INFO : PROGRESS: at sentence #360000, processed 2825783 words and 1336932 word types\n",
      "2023-06-27 18:10:54,968 : INFO : PROGRESS: at sentence #370000, processed 2905331 words and 1367579 word types\n",
      "2023-06-27 18:10:55,079 : INFO : PROGRESS: at sentence #380000, processed 2984396 words and 1397193 word types\n",
      "2023-06-27 18:10:55,270 : INFO : PROGRESS: at sentence #390000, processed 3062259 words and 1426273 word types\n",
      "2023-06-27 18:10:55,373 : INFO : PROGRESS: at sentence #400000, processed 3141362 words and 1455830 word types\n",
      "2023-06-27 18:10:55,478 : INFO : PROGRESS: at sentence #410000, processed 3220146 words and 1485538 word types\n",
      "2023-06-27 18:10:55,581 : INFO : PROGRESS: at sentence #420000, processed 3298424 words and 1513779 word types\n",
      "2023-06-27 18:10:55,683 : INFO : PROGRESS: at sentence #430000, processed 3376131 words and 1542020 word types\n",
      "2023-06-27 18:10:55,789 : INFO : PROGRESS: at sentence #440000, processed 3455218 words and 1570837 word types\n",
      "2023-06-27 18:10:55,893 : INFO : PROGRESS: at sentence #450000, processed 3534205 words and 1600218 word types\n",
      "2023-06-27 18:10:55,995 : INFO : PROGRESS: at sentence #460000, processed 3613496 words and 1628805 word types\n",
      "2023-06-27 18:10:56,101 : INFO : PROGRESS: at sentence #470000, processed 3691632 words and 1656911 word types\n",
      "2023-06-27 18:10:56,207 : INFO : PROGRESS: at sentence #480000, processed 3771374 words and 1685184 word types\n",
      "2023-06-27 18:10:56,314 : INFO : PROGRESS: at sentence #490000, processed 3851520 words and 1714917 word types\n",
      "2023-06-27 18:10:56,416 : INFO : PROGRESS: at sentence #500000, processed 3930457 words and 1743281 word types\n",
      "2023-06-27 18:10:56,524 : INFO : PROGRESS: at sentence #510000, processed 4009287 words and 1770985 word types\n",
      "2023-06-27 18:10:56,634 : INFO : PROGRESS: at sentence #520000, processed 4088571 words and 1798862 word types\n",
      "2023-06-27 18:10:56,748 : INFO : PROGRESS: at sentence #530000, processed 4169204 words and 1828003 word types\n",
      "2023-06-27 18:10:56,873 : INFO : PROGRESS: at sentence #540000, processed 4248983 words and 1855827 word types\n",
      "2023-06-27 18:10:56,986 : INFO : PROGRESS: at sentence #550000, processed 4328316 words and 1882865 word types\n",
      "2023-06-27 18:10:57,097 : INFO : PROGRESS: at sentence #560000, processed 4408907 words and 1910413 word types\n",
      "2023-06-27 18:10:57,222 : INFO : PROGRESS: at sentence #570000, processed 4488545 words and 1938699 word types\n",
      "2023-06-27 18:10:57,335 : INFO : PROGRESS: at sentence #580000, processed 4568107 words and 1965968 word types\n",
      "2023-06-27 18:10:57,439 : INFO : PROGRESS: at sentence #590000, processed 4647596 words and 1992815 word types\n",
      "2023-06-27 18:10:57,551 : INFO : PROGRESS: at sentence #600000, processed 4727398 words and 2019842 word types\n",
      "2023-06-27 18:10:57,670 : INFO : PROGRESS: at sentence #610000, processed 4808516 words and 2048461 word types\n",
      "2023-06-27 18:10:57,778 : INFO : PROGRESS: at sentence #620000, processed 4888824 words and 2075931 word types\n",
      "2023-06-27 18:10:57,886 : INFO : PROGRESS: at sentence #630000, processed 4968609 words and 2102933 word types\n",
      "2023-06-27 18:10:57,993 : INFO : PROGRESS: at sentence #640000, processed 5047391 words and 2129411 word types\n",
      "2023-06-27 18:10:58,101 : INFO : PROGRESS: at sentence #650000, processed 5127982 words and 2157076 word types\n",
      "2023-06-27 18:10:58,216 : INFO : PROGRESS: at sentence #660000, processed 5207471 words and 2183449 word types\n",
      "2023-06-27 18:10:58,320 : INFO : PROGRESS: at sentence #670000, processed 5285506 words and 2208837 word types\n",
      "2023-06-27 18:10:58,424 : INFO : PROGRESS: at sentence #680000, processed 5364187 words and 2234508 word types\n",
      "2023-06-27 18:10:58,532 : INFO : PROGRESS: at sentence #690000, processed 5444310 words and 2260965 word types\n",
      "2023-06-27 18:10:58,642 : INFO : PROGRESS: at sentence #700000, processed 5524112 words and 2287088 word types\n",
      "2023-06-27 18:10:58,754 : INFO : PROGRESS: at sentence #710000, processed 5603838 words and 2312643 word types\n",
      "2023-06-27 18:10:58,867 : INFO : PROGRESS: at sentence #720000, processed 5683549 words and 2339041 word types\n",
      "2023-06-27 18:10:58,974 : INFO : PROGRESS: at sentence #730000, processed 5762853 words and 2365192 word types\n",
      "2023-06-27 18:10:59,087 : INFO : PROGRESS: at sentence #740000, processed 5843874 words and 2392378 word types\n",
      "2023-06-27 18:10:59,194 : INFO : PROGRESS: at sentence #750000, processed 5923404 words and 2418794 word types\n",
      "2023-06-27 18:10:59,310 : INFO : PROGRESS: at sentence #760000, processed 5998336 words and 2448658 word types\n",
      "2023-06-27 18:10:59,411 : INFO : PROGRESS: at sentence #770000, processed 6071169 words and 2475994 word types\n",
      "2023-06-27 18:10:59,510 : INFO : PROGRESS: at sentence #780000, processed 6144788 words and 2503606 word types\n",
      "2023-06-27 18:10:59,613 : INFO : PROGRESS: at sentence #790000, processed 6217874 words and 2531233 word types\n",
      "2023-06-27 18:10:59,721 : INFO : PROGRESS: at sentence #800000, processed 6293544 words and 2559829 word types\n",
      "2023-06-27 18:10:59,825 : INFO : PROGRESS: at sentence #810000, processed 6369161 words and 2588842 word types\n",
      "2023-06-27 18:10:59,941 : INFO : PROGRESS: at sentence #820000, processed 6444402 words and 2616897 word types\n",
      "2023-06-27 18:11:00,054 : INFO : PROGRESS: at sentence #830000, processed 6519410 words and 2644700 word types\n",
      "2023-06-27 18:11:00,157 : INFO : PROGRESS: at sentence #840000, processed 6593612 words and 2671081 word types\n",
      "2023-06-27 18:11:00,274 : INFO : PROGRESS: at sentence #850000, processed 6669199 words and 2698803 word types\n",
      "2023-06-27 18:11:00,383 : INFO : PROGRESS: at sentence #860000, processed 6744435 words and 2725193 word types\n",
      "2023-06-27 18:11:00,494 : INFO : PROGRESS: at sentence #870000, processed 6820954 words and 2751400 word types\n",
      "2023-06-27 18:11:00,603 : INFO : PROGRESS: at sentence #880000, processed 6896864 words and 2779241 word types\n",
      "2023-06-27 18:11:00,905 : INFO : PROGRESS: at sentence #890000, processed 6971611 words and 2806291 word types\n",
      "2023-06-27 18:11:01,006 : INFO : PROGRESS: at sentence #900000, processed 7047211 words and 2833837 word types\n",
      "2023-06-27 18:11:01,113 : INFO : PROGRESS: at sentence #910000, processed 7121069 words and 2860208 word types\n",
      "2023-06-27 18:11:01,217 : INFO : PROGRESS: at sentence #920000, processed 7198259 words and 2888517 word types\n",
      "2023-06-27 18:11:01,320 : INFO : PROGRESS: at sentence #930000, processed 7274692 words and 2916323 word types\n",
      "2023-06-27 18:11:01,421 : INFO : PROGRESS: at sentence #940000, processed 7350346 words and 2943383 word types\n",
      "2023-06-27 18:11:01,522 : INFO : PROGRESS: at sentence #950000, processed 7425485 words and 2970921 word types\n",
      "2023-06-27 18:11:01,623 : INFO : PROGRESS: at sentence #960000, processed 7501002 words and 2996878 word types\n",
      "2023-06-27 18:11:01,726 : INFO : PROGRESS: at sentence #970000, processed 7576018 words and 3023466 word types\n",
      "2023-06-27 18:11:01,830 : INFO : PROGRESS: at sentence #980000, processed 7651336 words and 3050205 word types\n",
      "2023-06-27 18:11:01,933 : INFO : PROGRESS: at sentence #990000, processed 7725692 words and 3075823 word types\n",
      "2023-06-27 18:11:02,037 : INFO : PROGRESS: at sentence #1000000, processed 7799442 words and 3101448 word types\n",
      "2023-06-27 18:11:02,136 : INFO : PROGRESS: at sentence #1010000, processed 7873085 words and 3126445 word types\n",
      "2023-06-27 18:11:02,243 : INFO : PROGRESS: at sentence #1020000, processed 7949365 words and 3152923 word types\n",
      "2023-06-27 18:11:02,345 : INFO : PROGRESS: at sentence #1030000, processed 8024999 words and 3179590 word types\n",
      "2023-06-27 18:11:02,447 : INFO : PROGRESS: at sentence #1040000, processed 8100101 words and 3205393 word types\n",
      "2023-06-27 18:11:02,552 : INFO : PROGRESS: at sentence #1050000, processed 8172998 words and 3230007 word types\n",
      "2023-06-27 18:11:02,655 : INFO : PROGRESS: at sentence #1060000, processed 8247463 words and 3255544 word types\n",
      "2023-06-27 18:11:02,754 : INFO : PROGRESS: at sentence #1070000, processed 8321850 words and 3280518 word types\n",
      "2023-06-27 18:11:02,861 : INFO : PROGRESS: at sentence #1080000, processed 8398406 words and 3306700 word types\n",
      "2023-06-27 18:11:02,970 : INFO : PROGRESS: at sentence #1090000, processed 8474432 words and 3332887 word types\n",
      "2023-06-27 18:11:03,073 : INFO : PROGRESS: at sentence #1100000, processed 8550534 words and 3358311 word types\n",
      "2023-06-27 18:11:03,176 : INFO : PROGRESS: at sentence #1110000, processed 8625271 words and 3382873 word types\n",
      "2023-06-27 18:11:03,280 : INFO : PROGRESS: at sentence #1120000, processed 8700390 words and 3407422 word types\n",
      "2023-06-27 18:11:03,382 : INFO : PROGRESS: at sentence #1130000, processed 8775988 words and 3432798 word types\n",
      "2023-06-27 18:11:03,487 : INFO : PROGRESS: at sentence #1140000, processed 8852365 words and 3459596 word types\n",
      "2023-06-27 18:11:03,605 : INFO : PROGRESS: at sentence #1150000, processed 8929100 words and 3485560 word types\n",
      "2023-06-27 18:11:03,709 : INFO : PROGRESS: at sentence #1160000, processed 9004233 words and 3510695 word types\n",
      "2023-06-27 18:11:03,815 : INFO : PROGRESS: at sentence #1170000, processed 9078576 words and 3535292 word types\n",
      "2023-06-27 18:11:03,918 : INFO : PROGRESS: at sentence #1180000, processed 9153579 words and 3560136 word types\n",
      "2023-06-27 18:11:04,025 : INFO : PROGRESS: at sentence #1190000, processed 9229615 words and 3585418 word types\n",
      "2023-06-27 18:11:04,131 : INFO : PROGRESS: at sentence #1200000, processed 9305745 words and 3612234 word types\n",
      "2023-06-27 18:11:04,242 : INFO : PROGRESS: at sentence #1210000, processed 9381957 words and 3637823 word types\n",
      "2023-06-27 18:11:04,350 : INFO : PROGRESS: at sentence #1220000, processed 9457951 words and 3662926 word types\n",
      "2023-06-27 18:11:04,455 : INFO : PROGRESS: at sentence #1230000, processed 9534604 words and 3689179 word types\n",
      "2023-06-27 18:11:04,561 : INFO : PROGRESS: at sentence #1240000, processed 9610718 words and 3715272 word types\n",
      "2023-06-27 18:11:04,671 : INFO : PROGRESS: at sentence #1250000, processed 9686532 words and 3740226 word types\n",
      "2023-06-27 18:11:04,775 : INFO : PROGRESS: at sentence #1260000, processed 9761036 words and 3764932 word types\n",
      "2023-06-27 18:11:04,883 : INFO : PROGRESS: at sentence #1270000, processed 9835492 words and 3788620 word types\n",
      "2023-06-27 18:11:04,985 : INFO : PROGRESS: at sentence #1280000, processed 9910521 words and 3812642 word types\n",
      "2023-06-27 18:11:05,101 : INFO : PROGRESS: at sentence #1290000, processed 9987324 words and 3837818 word types\n",
      "2023-06-27 18:11:05,212 : INFO : PROGRESS: at sentence #1300000, processed 10063407 words and 3863292 word types\n",
      "2023-06-27 18:11:05,320 : INFO : PROGRESS: at sentence #1310000, processed 10138206 words and 3887506 word types\n",
      "2023-06-27 18:11:05,432 : INFO : PROGRESS: at sentence #1320000, processed 10212678 words and 3911551 word types\n",
      "2023-06-27 18:11:05,555 : INFO : PROGRESS: at sentence #1330000, processed 10286398 words and 3935397 word types\n",
      "2023-06-27 18:11:05,665 : INFO : PROGRESS: at sentence #1340000, processed 10361143 words and 3958829 word types\n",
      "2023-06-27 18:11:05,771 : INFO : PROGRESS: at sentence #1350000, processed 10437565 words and 3983452 word types\n",
      "2023-06-27 18:11:05,876 : INFO : PROGRESS: at sentence #1360000, processed 10513126 words and 4008527 word types\n",
      "2023-06-27 18:11:05,983 : INFO : PROGRESS: at sentence #1370000, processed 10589271 words and 4032652 word types\n",
      "2023-06-27 18:11:06,113 : INFO : PROGRESS: at sentence #1380000, processed 10663634 words and 4056253 word types\n",
      "2023-06-27 18:11:06,225 : INFO : PROGRESS: at sentence #1390000, processed 10738034 words and 4079774 word types\n",
      "2023-06-27 18:11:06,323 : INFO : PROGRESS: at sentence #1400000, processed 10812621 words and 4102655 word types\n",
      "2023-06-27 18:11:06,431 : INFO : PROGRESS: at sentence #1410000, processed 10889736 words and 4128206 word types\n",
      "2023-06-27 18:11:06,537 : INFO : PROGRESS: at sentence #1420000, processed 10966046 words and 4152957 word types\n",
      "2023-06-27 18:11:06,652 : INFO : PROGRESS: at sentence #1430000, processed 11041535 words and 4176791 word types\n",
      "2023-06-27 18:11:06,769 : INFO : PROGRESS: at sentence #1440000, processed 11116419 words and 4200089 word types\n",
      "2023-06-27 18:11:06,878 : INFO : PROGRESS: at sentence #1450000, processed 11191187 words and 4223592 word types\n",
      "2023-06-27 18:11:06,987 : INFO : PROGRESS: at sentence #1460000, processed 11266942 words and 4247375 word types\n",
      "2023-06-27 18:11:07,103 : INFO : PROGRESS: at sentence #1470000, processed 11343405 words and 4272418 word types\n",
      "2023-06-27 18:11:07,223 : INFO : PROGRESS: at sentence #1480000, processed 11419672 words and 4296471 word types\n",
      "2023-06-27 18:11:07,256 : INFO : collected 4303533 token types (unigram + bigrams) from a corpus of 11441979 words and 1482941 sentences\n",
      "2023-06-27 18:11:07,257 : INFO : merged Phrases<4303533 vocab, min_count=40, threshold=10.0, max_vocab_size=40000000>\n",
      "2023-06-27 18:11:07,259 : INFO : Phrases lifecycle event {'msg': 'built Phrases<4303533 vocab, min_count=40, threshold=10.0, max_vocab_size=40000000> in 16.51s', 'datetime': '2023-06-27T18:11:07.259743', 'gensim': '4.3.0', 'python': '3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n",
      "2023-06-27 18:11:07,259 : INFO : exporting phrases from Phrases<4303533 vocab, min_count=40, threshold=10.0, max_vocab_size=40000000>\n",
      "2023-06-27 18:11:16,470 : INFO : FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<1806 phrases, min_count=40, threshold=10.0> from Phrases<4303533 vocab, min_count=40, threshold=10.0, max_vocab_size=40000000> in 9.21s', 'datetime': '2023-06-27T18:11:16.470794', 'gensim': '4.3.0', 'python': '3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(unigrams, min_count=40, progress_per=30000)\n",
    "bigrams = Phraser(phrases)\n",
    "sentences = bigrams[unigrams]\n",
    "# sentences[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454515"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check time ! im not schizo\n",
    "# get the number of words in the vocabulary\n",
    "\n",
    "wrdfreq = defaultdict(int) \n",
    "for myphrase in sentences:\n",
    "    for oneword in myphrase:\n",
    "        wrdfreq[oneword] += 1\n",
    "len(wrdfreq) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['im', 'go', 'get', 'day', 'work', 'love', 'good', 'like', 'today', 'time']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the most frequent words\n",
    "# we removed stopwords, so not seen 'the' is expected\n",
    "sorted(wrdfreq, key=wrdfreq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for workers in building genshimmodel\n",
    "\n",
    "\n",
    "corecount = multiprocessing.cpu_count()\n",
    "corecount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 18:12:56,156 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.04>', 'datetime': '2023-06-27T18:12:56.156268', 'gensim': '4.3.0', 'python': '3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "genshinmodel = Word2Vec(\n",
    "    min_count= 30, # ignores words which appear less than 30 times in the corpora\n",
    "    window= 7, # context window size\n",
    "    vector_size= 300, # size of the vector\n",
    "    sample= 5e-5, # random downsampling of high freq words\n",
    "    alpha= 0.04, # learning rate\n",
    "    min_alpha= 0.005, # minimum rate of learning, where it will stop dropping\n",
    "    negative= 10, # negative sampling for drowning\n",
    "    workers= corecount-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 18:13:06,481 : INFO : collecting all words and their counts\n",
      "2023-06-27 18:13:06,481 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-06-27 18:13:06,721 : INFO : PROGRESS: at sentence #30000, processed 218763 words, keeping 25576 word types\n",
      "2023-06-27 18:13:06,966 : INFO : PROGRESS: at sentence #60000, processed 442745 words, keeping 40768 word types\n",
      "2023-06-27 18:13:07,236 : INFO : PROGRESS: at sentence #90000, processed 666765 words, keeping 53370 word types\n",
      "2023-06-27 18:13:07,498 : INFO : PROGRESS: at sentence #120000, processed 892270 words, keeping 65389 word types\n",
      "2023-06-27 18:13:07,778 : INFO : PROGRESS: at sentence #150000, processed 1120229 words, keeping 76654 word types\n",
      "2023-06-27 18:13:08,035 : INFO : PROGRESS: at sentence #180000, processed 1345897 words, keeping 87032 word types\n",
      "2023-06-27 18:13:08,312 : INFO : PROGRESS: at sentence #210000, processed 1572142 words, keeping 97157 word types\n",
      "2023-06-27 18:13:08,569 : INFO : PROGRESS: at sentence #240000, processed 1797299 words, keeping 106688 word types\n",
      "2023-06-27 18:13:08,832 : INFO : PROGRESS: at sentence #270000, processed 2024011 words, keeping 115996 word types\n",
      "2023-06-27 18:13:09,091 : INFO : PROGRESS: at sentence #300000, processed 2250228 words, keeping 124958 word types\n",
      "2023-06-27 18:13:09,357 : INFO : PROGRESS: at sentence #330000, processed 2479353 words, keeping 134220 word types\n",
      "2023-06-27 18:13:09,631 : INFO : PROGRESS: at sentence #360000, processed 2706201 words, keeping 142812 word types\n",
      "2023-06-27 18:13:09,892 : INFO : PROGRESS: at sentence #390000, processed 2932557 words, keeping 151478 word types\n",
      "2023-06-27 18:13:10,146 : INFO : PROGRESS: at sentence #420000, processed 3158823 words, keeping 160067 word types\n",
      "2023-06-27 18:13:10,420 : INFO : PROGRESS: at sentence #450000, processed 3385259 words, keeping 168397 word types\n",
      "2023-06-27 18:13:10,715 : INFO : PROGRESS: at sentence #480000, processed 3612740 words, keeping 176359 word types\n",
      "2023-06-27 18:13:10,979 : INFO : PROGRESS: at sentence #510000, processed 3840957 words, keeping 184431 word types\n",
      "2023-06-27 18:13:11,237 : INFO : PROGRESS: at sentence #540000, processed 4070675 words, keeping 192399 word types\n",
      "2023-06-27 18:13:11,505 : INFO : PROGRESS: at sentence #570000, processed 4300485 words, keeping 200198 word types\n",
      "2023-06-27 18:13:11,776 : INFO : PROGRESS: at sentence #600000, processed 4529656 words, keeping 207648 word types\n",
      "2023-06-27 18:13:12,043 : INFO : PROGRESS: at sentence #630000, processed 4761110 words, keeping 215608 word types\n",
      "2023-06-27 18:13:12,305 : INFO : PROGRESS: at sentence #660000, processed 4990019 words, keeping 223486 word types\n",
      "2023-06-27 18:13:12,568 : INFO : PROGRESS: at sentence #690000, processed 5216824 words, keeping 230864 word types\n",
      "2023-06-27 18:13:12,836 : INFO : PROGRESS: at sentence #720000, processed 5445273 words, keeping 238230 word types\n",
      "2023-06-27 18:13:13,127 : INFO : PROGRESS: at sentence #750000, processed 5675113 words, keeping 245920 word types\n",
      "2023-06-27 18:13:13,385 : INFO : PROGRESS: at sentence #780000, processed 5888168 words, keeping 254972 word types\n",
      "2023-06-27 18:13:13,647 : INFO : PROGRESS: at sentence #810000, processed 6104071 words, keeping 264215 word types\n",
      "2023-06-27 18:13:13,904 : INFO : PROGRESS: at sentence #840000, processed 6319699 words, keeping 273197 word types\n",
      "2023-06-27 18:13:14,169 : INFO : PROGRESS: at sentence #870000, processed 6536645 words, keeping 281941 word types\n",
      "2023-06-27 18:13:14,434 : INFO : PROGRESS: at sentence #900000, processed 6754088 words, keeping 291004 word types\n",
      "2023-06-27 18:13:14,694 : INFO : PROGRESS: at sentence #930000, processed 6973079 words, keeping 300180 word types\n",
      "2023-06-27 18:13:14,972 : INFO : PROGRESS: at sentence #960000, processed 7190744 words, keeping 309076 word types\n",
      "2023-06-27 18:13:15,240 : INFO : PROGRESS: at sentence #990000, processed 7406972 words, keeping 317668 word types\n",
      "2023-06-27 18:13:15,508 : INFO : PROGRESS: at sentence #1020000, processed 7621953 words, keeping 326118 word types\n",
      "2023-06-27 18:13:15,786 : INFO : PROGRESS: at sentence #1050000, processed 7836172 words, keeping 334639 word types\n",
      "2023-06-27 18:13:16,056 : INFO : PROGRESS: at sentence #1080000, processed 8052798 words, keeping 343046 word types\n",
      "2023-06-27 18:13:16,326 : INFO : PROGRESS: at sentence #1110000, processed 8270100 words, keeping 351422 word types\n",
      "2023-06-27 18:13:16,579 : INFO : PROGRESS: at sentence #1140000, processed 8487045 words, keeping 360115 word types\n",
      "2023-06-27 18:13:16,843 : INFO : PROGRESS: at sentence #1170000, processed 8704372 words, keeping 368504 word types\n",
      "2023-06-27 18:13:17,097 : INFO : PROGRESS: at sentence #1200000, processed 8923033 words, keeping 377177 word types\n",
      "2023-06-27 18:13:17,353 : INFO : PROGRESS: at sentence #1230000, processed 9143415 words, keeping 385852 word types\n",
      "2023-06-27 18:13:17,609 : INFO : PROGRESS: at sentence #1260000, processed 9360966 words, keeping 394493 word types\n",
      "2023-06-27 18:13:17,865 : INFO : PROGRESS: at sentence #1290000, processed 9578452 words, keeping 402558 word types\n",
      "2023-06-27 18:13:18,121 : INFO : PROGRESS: at sentence #1320000, processed 9794692 words, keeping 410840 word types\n",
      "2023-06-27 18:13:18,379 : INFO : PROGRESS: at sentence #1350000, processed 10010887 words, keeping 418849 word types\n",
      "2023-06-27 18:13:18,634 : INFO : PROGRESS: at sentence #1380000, processed 10227791 words, keeping 427036 word types\n",
      "2023-06-27 18:13:18,891 : INFO : PROGRESS: at sentence #1410000, processed 10445213 words, keeping 435030 word types\n",
      "2023-06-27 18:13:19,149 : INFO : PROGRESS: at sentence #1440000, processed 10663272 words, keeping 442931 word types\n",
      "2023-06-27 18:13:19,405 : INFO : PROGRESS: at sentence #1470000, processed 10881657 words, keeping 450959 word types\n",
      "2023-06-27 18:13:19,513 : INFO : collected 454515 word types from a corpus of 10976342 raw words and 1482941 sentences\n",
      "2023-06-27 18:13:19,514 : INFO : Creating a fresh vocabulary\n",
      "2023-06-27 18:13:19,710 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=30 retains 15590 unique words (3.43% of original 454515, drops 438925)', 'datetime': '2023-06-27T18:13:19.710665', 'gensim': '4.3.0', 'python': '3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-06-27 18:13:19,711 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=30 leaves 10110322 word corpus (92.11% of original 10976342, drops 866020)', 'datetime': '2023-06-27T18:13:19.711665', 'gensim': '4.3.0', 'python': '3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-06-27 18:13:19,787 : INFO : deleting the raw counts dictionary of 454515 items\n",
      "2023-06-27 18:13:19,804 : INFO : sample=5e-05 downsamples 1090 most-common words\n",
      "2023-06-27 18:13:19,805 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4863974.448549687 word corpus (48.1%% of prior 10110322)', 'datetime': '2023-06-27T18:13:19.805743', 'gensim': '4.3.0', 'python': '3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-06-27 18:13:19,916 : INFO : estimated required memory for 15590 words and 300 dimensions: 45211000 bytes\n",
      "2023-06-27 18:13:19,917 : INFO : resetting layer weights\n",
      "2023-06-27 18:13:19,941 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-06-27T18:13:19.941648', 'gensim': '4.3.0', 'python': '3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "took time 13.461478471755981 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = time.time()\n",
    "\n",
    "# monitor time and build the vocabulary \n",
    "# mesh them up real good\n",
    "genshinmodel.build_vocab(sentences, progress_per=30000)\n",
    "print(\"\\n\\ntook time\", (time.time()-t), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 11:37:11,006 : INFO : Word2Vec lifecycle event {'msg': 'training model with 14 workers on 15590 vocabulary and 300 features, using sg=0 hs=0 sample=5e-05 negative=10 window=7 shrink_windows=True', 'datetime': '2023-06-27T11:37:11.006274', 'gensim': '4.3.0', 'python': '3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2023-06-27 11:37:12,055 : INFO : EPOCH 0 - PROGRESS: at 5.11% examples, 240938 words/s, in_qsize 23, out_qsize 1\n",
      "2023-06-27 11:37:13,065 : INFO : EPOCH 0 - PROGRESS: at 11.90% examples, 286875 words/s, in_qsize 28, out_qsize 2\n",
      "2023-06-27 11:37:14,805 : INFO : EPOCH 0 - PROGRESS: at 15.66% examples, 203855 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:37:15,828 : INFO : EPOCH 0 - PROGRESS: at 22.87% examples, 235432 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:37:16,832 : INFO : EPOCH 0 - PROGRESS: at 29.93% examples, 254644 words/s, in_qsize 24, out_qsize 3\n",
      "2023-06-27 11:37:17,844 : INFO : EPOCH 0 - PROGRESS: at 36.75% examples, 267009 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:37:18,916 : INFO : EPOCH 0 - PROGRESS: at 43.87% examples, 276057 words/s, in_qsize 28, out_qsize 5\n",
      "2023-06-27 11:37:19,925 : INFO : EPOCH 0 - PROGRESS: at 50.97% examples, 284610 words/s, in_qsize 28, out_qsize 2\n",
      "2023-06-27 11:37:21,202 : INFO : EPOCH 0 - PROGRESS: at 51.92% examples, 253303 words/s, in_qsize 24, out_qsize 4\n",
      "2023-06-27 11:37:22,208 : INFO : EPOCH 0 - PROGRESS: at 59.32% examples, 261813 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:37:23,224 : INFO : EPOCH 0 - PROGRESS: at 66.66% examples, 268604 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:37:24,252 : INFO : EPOCH 0 - PROGRESS: at 74.16% examples, 274555 words/s, in_qsize 22, out_qsize 5\n",
      "2023-06-27 11:37:25,267 : INFO : EPOCH 0 - PROGRESS: at 81.59% examples, 280016 words/s, in_qsize 24, out_qsize 3\n",
      "2023-06-27 11:37:27,339 : INFO : EPOCH 0 - PROGRESS: at 88.27% examples, 263971 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:37:28,349 : INFO : EPOCH 0 - PROGRESS: at 95.73% examples, 269057 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:37:28,564 : INFO : EPOCH 0: training on 10976342 raw words (4864355 effective words) took 17.5s, 277419 effective words/s\n",
      "2023-06-27 11:37:29,626 : INFO : EPOCH 1 - PROGRESS: at 4.66% examples, 214837 words/s, in_qsize 28, out_qsize 5\n",
      "2023-06-27 11:37:30,635 : INFO : EPOCH 1 - PROGRESS: at 12.08% examples, 287356 words/s, in_qsize 28, out_qsize 4\n",
      "2023-06-27 11:37:31,640 : INFO : EPOCH 1 - PROGRESS: at 19.60% examples, 314940 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:37:33,501 : INFO : EPOCH 1 - PROGRESS: at 25.01% examples, 250747 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:37:34,513 : INFO : EPOCH 1 - PROGRESS: at 32.24% examples, 268232 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:37:35,524 : INFO : EPOCH 1 - PROGRESS: at 39.39% examples, 280875 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:37:36,547 : INFO : EPOCH 1 - PROGRESS: at 46.28% examples, 287932 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:37:37,557 : INFO : EPOCH 1 - PROGRESS: at 53.73% examples, 296141 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:37:38,560 : INFO : EPOCH 1 - PROGRESS: at 61.19% examples, 301838 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:37:39,775 : INFO : EPOCH 1 - PROGRESS: at 61.28% examples, 269496 words/s, in_qsize 28, out_qsize 6\n",
      "2023-06-27 11:37:40,784 : INFO : EPOCH 1 - PROGRESS: at 68.83% examples, 276485 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:37:41,833 : INFO : EPOCH 1 - PROGRESS: at 76.32% examples, 281577 words/s, in_qsize 28, out_qsize 3\n",
      "2023-06-27 11:37:42,847 : INFO : EPOCH 1 - PROGRESS: at 83.89% examples, 287036 words/s, in_qsize 27, out_qsize 4\n",
      "2023-06-27 11:37:43,852 : INFO : EPOCH 1 - PROGRESS: at 91.45% examples, 291635 words/s, in_qsize 22, out_qsize 6\n",
      "2023-06-27 11:37:44,610 : INFO : EPOCH 1: training on 10976342 raw words (4864308 effective words) took 16.0s, 303366 effective words/s\n",
      "2023-06-27 11:37:46,185 : INFO : EPOCH 2 - PROGRESS: at 0.18% examples, 5801 words/s, in_qsize 26, out_qsize 0\n",
      "2023-06-27 11:37:47,190 : INFO : EPOCH 2 - PROGRESS: at 7.18% examples, 136946 words/s, in_qsize 23, out_qsize 4\n",
      "2023-06-27 11:37:48,216 : INFO : EPOCH 2 - PROGRESS: at 14.59% examples, 200218 words/s, in_qsize 26, out_qsize 3\n",
      "2023-06-27 11:37:49,222 : INFO : EPOCH 2 - PROGRESS: at 21.99% examples, 236859 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:37:50,226 : INFO : EPOCH 2 - PROGRESS: at 28.41% examples, 251085 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:37:52,454 : INFO : EPOCH 2 - PROGRESS: at 36.04% examples, 228261 words/s, in_qsize 25, out_qsize 0\n",
      "2023-06-27 11:37:53,478 : INFO : EPOCH 2 - PROGRESS: at 42.55% examples, 238839 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:37:54,481 : INFO : EPOCH 2 - PROGRESS: at 49.19% examples, 248191 words/s, in_qsize 23, out_qsize 4\n",
      "2023-06-27 11:37:55,489 : INFO : EPOCH 2 - PROGRESS: at 56.06% examples, 255364 words/s, in_qsize 28, out_qsize 3\n",
      "2023-06-27 11:37:56,491 : INFO : EPOCH 2 - PROGRESS: at 63.21% examples, 262587 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:37:57,509 : INFO : EPOCH 2 - PROGRESS: at 70.43% examples, 268234 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:37:59,048 : INFO : EPOCH 2 - PROGRESS: at 72.41% examples, 246093 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:38:00,084 : INFO : EPOCH 2 - PROGRESS: at 78.82% examples, 249460 words/s, in_qsize 19, out_qsize 8\n",
      "2023-06-27 11:38:01,127 : INFO : EPOCH 2 - PROGRESS: at 86.51% examples, 255987 words/s, in_qsize 22, out_qsize 2\n",
      "2023-06-27 11:38:02,162 : INFO : EPOCH 2 - PROGRESS: at 93.79% examples, 260637 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:38:02,622 : INFO : EPOCH 2: training on 10976342 raw words (4864362 effective words) took 18.0s, 270499 effective words/s\n",
      "2023-06-27 11:38:03,654 : INFO : EPOCH 3 - PROGRESS: at 4.75% examples, 228025 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:38:05,378 : INFO : EPOCH 3 - PROGRESS: at 8.78% examples, 156786 words/s, in_qsize 26, out_qsize 3\n",
      "2023-06-27 11:38:06,381 : INFO : EPOCH 3 - PROGRESS: at 15.84% examples, 208158 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:38:07,394 : INFO : EPOCH 3 - PROGRESS: at 22.43% examples, 233168 words/s, in_qsize 27, out_qsize 4\n",
      "2023-06-27 11:38:08,401 : INFO : EPOCH 3 - PROGRESS: at 29.84% examples, 255830 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:38:09,429 : INFO : EPOCH 3 - PROGRESS: at 36.48% examples, 266149 words/s, in_qsize 26, out_qsize 3\n",
      "2023-06-27 11:38:10,438 : INFO : EPOCH 3 - PROGRESS: at 43.52% examples, 277018 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:38:11,863 : INFO : EPOCH 3 - PROGRESS: at 45.47% examples, 244705 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:38:12,879 : INFO : EPOCH 3 - PROGRESS: at 52.31% examples, 253354 words/s, in_qsize 26, out_qsize 2\n",
      "2023-06-27 11:38:13,885 : INFO : EPOCH 3 - PROGRESS: at 59.79% examples, 262197 words/s, in_qsize 25, out_qsize 3\n",
      "2023-06-27 11:38:14,890 : INFO : EPOCH 3 - PROGRESS: at 66.85% examples, 268105 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:38:15,950 : INFO : EPOCH 3 - PROGRESS: at 74.53% examples, 274109 words/s, in_qsize 28, out_qsize 2\n",
      "2023-06-27 11:38:18,067 : INFO : EPOCH 3 - PROGRESS: at 81.69% examples, 258730 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:38:19,078 : INFO : EPOCH 3 - PROGRESS: at 88.65% examples, 263021 words/s, in_qsize 24, out_qsize 3\n",
      "2023-06-27 11:38:20,121 : INFO : EPOCH 3 - PROGRESS: at 95.63% examples, 266369 words/s, in_qsize 27, out_qsize 2\n",
      "2023-06-27 11:38:20,365 : INFO : EPOCH 3: training on 10976342 raw words (4863956 effective words) took 17.7s, 274506 effective words/s\n",
      "2023-06-27 11:38:21,415 : INFO : EPOCH 4 - PROGRESS: at 4.30% examples, 202864 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:38:22,433 : INFO : EPOCH 4 - PROGRESS: at 11.00% examples, 263670 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:38:23,455 : INFO : EPOCH 4 - PROGRESS: at 17.81% examples, 285490 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:38:24,796 : INFO : EPOCH 4 - PROGRESS: at 18.70% examples, 208833 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:38:25,798 : INFO : EPOCH 4 - PROGRESS: at 25.54% examples, 233291 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:38:26,826 : INFO : EPOCH 4 - PROGRESS: at 32.86% examples, 252174 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:38:27,832 : INFO : EPOCH 4 - PROGRESS: at 39.74% examples, 264560 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:38:28,839 : INFO : EPOCH 4 - PROGRESS: at 46.63% examples, 273705 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:38:29,862 : INFO : EPOCH 4 - PROGRESS: at 53.82% examples, 281173 words/s, in_qsize 27, out_qsize 2\n",
      "2023-06-27 11:38:31,193 : INFO : EPOCH 4 - PROGRESS: at 55.49% examples, 253945 words/s, in_qsize 23, out_qsize 3\n",
      "2023-06-27 11:38:32,212 : INFO : EPOCH 4 - PROGRESS: at 63.03% examples, 262330 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:38:33,241 : INFO : EPOCH 4 - PROGRESS: at 70.62% examples, 269177 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:38:34,254 : INFO : EPOCH 4 - PROGRESS: at 77.60% examples, 273628 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:38:35,298 : INFO : EPOCH 4 - PROGRESS: at 85.39% examples, 279456 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:38:37,476 : INFO : EPOCH 4 - PROGRESS: at 92.29% examples, 262989 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:38:38,152 : INFO : EPOCH 4: training on 10976342 raw words (4862683 effective words) took 17.8s, 273754 effective words/s\n",
      "2023-06-27 11:38:39,205 : INFO : EPOCH 5 - PROGRESS: at 4.48% examples, 209770 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:38:40,232 : INFO : EPOCH 5 - PROGRESS: at 11.35% examples, 270518 words/s, in_qsize 27, out_qsize 1\n",
      "2023-06-27 11:38:41,260 : INFO : EPOCH 5 - PROGRESS: at 17.99% examples, 286634 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:38:42,267 : INFO : EPOCH 5 - PROGRESS: at 24.47% examples, 295223 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:38:44,266 : INFO : EPOCH 5 - PROGRESS: at 29.75% examples, 241117 words/s, in_qsize 26, out_qsize 2\n",
      "2023-06-27 11:38:45,305 : INFO : EPOCH 5 - PROGRESS: at 37.10% examples, 257627 words/s, in_qsize 27, out_qsize 2\n",
      "2023-06-27 11:38:46,332 : INFO : EPOCH 5 - PROGRESS: at 43.87% examples, 266779 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:38:47,336 : INFO : EPOCH 5 - PROGRESS: at 50.88% examples, 275805 words/s, in_qsize 27, out_qsize 2\n",
      "2023-06-27 11:38:48,342 : INFO : EPOCH 5 - PROGRESS: at 58.48% examples, 283803 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:38:49,355 : INFO : EPOCH 5 - PROGRESS: at 66.10% examples, 290507 words/s, in_qsize 24, out_qsize 2\n",
      "2023-06-27 11:38:50,646 : INFO : EPOCH 5 - PROGRESS: at 66.19% examples, 260781 words/s, in_qsize 28, out_qsize 2\n",
      "2023-06-27 11:38:51,659 : INFO : EPOCH 5 - PROGRESS: at 73.23% examples, 265925 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:38:52,668 : INFO : EPOCH 5 - PROGRESS: at 80.67% examples, 271983 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:38:53,724 : INFO : EPOCH 5 - PROGRESS: at 88.27% examples, 276866 words/s, in_qsize 27, out_qsize 1\n",
      "2023-06-27 11:38:54,754 : INFO : EPOCH 5 - PROGRESS: at 95.54% examples, 280527 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:38:54,968 : INFO : EPOCH 5: training on 10976342 raw words (4865359 effective words) took 16.8s, 289659 effective words/s\n",
      "2023-06-27 11:38:56,974 : INFO : EPOCH 6 - PROGRESS: at 3.39% examples, 82243 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:38:58,019 : INFO : EPOCH 6 - PROGRESS: at 11.18% examples, 180684 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:38:59,027 : INFO : EPOCH 6 - PROGRESS: at 18.08% examples, 220026 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:39:00,097 : INFO : EPOCH 6 - PROGRESS: at 25.10% examples, 242491 words/s, in_qsize 24, out_qsize 3\n",
      "2023-06-27 11:39:01,108 : INFO : EPOCH 6 - PROGRESS: at 32.16% examples, 259346 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:39:02,160 : INFO : EPOCH 6 - PROGRESS: at 39.12% examples, 270173 words/s, in_qsize 27, out_qsize 2\n",
      "2023-06-27 11:39:03,438 : INFO : EPOCH 6 - PROGRESS: at 39.92% examples, 234117 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:39:04,513 : INFO : EPOCH 6 - PROGRESS: at 47.07% examples, 245177 words/s, in_qsize 25, out_qsize 4\n",
      "2023-06-27 11:39:05,540 : INFO : EPOCH 6 - PROGRESS: at 54.19% examples, 254201 words/s, in_qsize 25, out_qsize 3\n",
      "2023-06-27 11:39:06,555 : INFO : EPOCH 6 - PROGRESS: at 60.91% examples, 259430 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:39:07,594 : INFO : EPOCH 6 - PROGRESS: at 68.00% examples, 264649 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:39:08,654 : INFO : EPOCH 6 - PROGRESS: at 75.38% examples, 269825 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:39:09,896 : INFO : EPOCH 6 - PROGRESS: at 76.22% examples, 250071 words/s, in_qsize 26, out_qsize 0\n",
      "2023-06-27 11:39:10,903 : INFO : EPOCH 6 - PROGRESS: at 82.88% examples, 254328 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:39:11,927 : INFO : EPOCH 6 - PROGRESS: at 90.06% examples, 259061 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:39:12,922 : INFO : EPOCH 6: training on 10976342 raw words (4864606 effective words) took 17.9s, 271158 effective words/s\n",
      "2023-06-27 11:39:13,994 : INFO : EPOCH 7 - PROGRESS: at 4.39% examples, 201928 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:39:15,005 : INFO : EPOCH 7 - PROGRESS: at 10.73% examples, 255280 words/s, in_qsize 27, out_qsize 4\n",
      "2023-06-27 11:39:16,563 : INFO : EPOCH 7 - PROGRESS: at 13.41% examples, 181885 words/s, in_qsize 23, out_qsize 1\n",
      "2023-06-27 11:39:17,635 : INFO : EPOCH 7 - PROGRESS: at 20.48% examples, 215094 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:39:18,656 : INFO : EPOCH 7 - PROGRESS: at 26.98% examples, 233120 words/s, in_qsize 22, out_qsize 5\n",
      "2023-06-27 11:39:19,671 : INFO : EPOCH 7 - PROGRESS: at 33.92% examples, 249202 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:39:20,688 : INFO : EPOCH 7 - PROGRESS: at 39.92% examples, 255330 words/s, in_qsize 28, out_qsize 2\n",
      "2023-06-27 11:39:21,691 : INFO : EPOCH 7 - PROGRESS: at 46.71% examples, 264959 words/s, in_qsize 27, out_qsize 2\n",
      "2023-06-27 11:39:23,457 : INFO : EPOCH 7 - PROGRESS: at 49.63% examples, 234352 words/s, in_qsize 23, out_qsize 4\n",
      "2023-06-27 11:39:24,469 : INFO : EPOCH 7 - PROGRESS: at 56.43% examples, 241963 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:39:25,495 : INFO : EPOCH 7 - PROGRESS: at 63.67% examples, 249652 words/s, in_qsize 25, out_qsize 1\n",
      "2023-06-27 11:39:26,513 : INFO : EPOCH 7 - PROGRESS: at 70.23% examples, 253714 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:39:27,540 : INFO : EPOCH 7 - PROGRESS: at 77.78% examples, 260618 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:39:28,562 : INFO : EPOCH 7 - PROGRESS: at 84.73% examples, 264883 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:39:30,005 : INFO : EPOCH 7 - PROGRESS: at 86.05% examples, 246109 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:39:31,023 : INFO : EPOCH 7 - PROGRESS: at 92.75% examples, 249875 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:39:31,646 : INFO : EPOCH 7: training on 10976342 raw words (4864126 effective words) took 18.7s, 260065 effective words/s\n",
      "2023-06-27 11:39:32,662 : INFO : EPOCH 8 - PROGRESS: at 4.30% examples, 207773 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:39:33,704 : INFO : EPOCH 8 - PROGRESS: at 11.54% examples, 276799 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:39:34,741 : INFO : EPOCH 8 - PROGRESS: at 18.88% examples, 301880 words/s, in_qsize 27, out_qsize 3\n",
      "2023-06-27 11:39:36,458 : INFO : EPOCH 8 - PROGRESS: at 23.23% examples, 239160 words/s, in_qsize 22, out_qsize 5\n",
      "2023-06-27 11:39:37,501 : INFO : EPOCH 8 - PROGRESS: at 30.73% examples, 259801 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:39:38,531 : INFO : EPOCH 8 - PROGRESS: at 38.24% examples, 275766 words/s, in_qsize 25, out_qsize 1\n",
      "2023-06-27 11:39:39,564 : INFO : EPOCH 8 - PROGRESS: at 45.38% examples, 284961 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:39:40,577 : INFO : EPOCH 8 - PROGRESS: at 52.98% examples, 294372 words/s, in_qsize 25, out_qsize 0\n",
      "2023-06-27 11:39:42,618 : INFO : EPOCH 8 - PROGRESS: at 59.32% examples, 266981 words/s, in_qsize 23, out_qsize 6\n",
      "2023-06-27 11:39:43,650 : INFO : EPOCH 8 - PROGRESS: at 67.04% examples, 274656 words/s, in_qsize 28, out_qsize 2\n",
      "2023-06-27 11:39:44,711 : INFO : EPOCH 8 - PROGRESS: at 74.72% examples, 280231 words/s, in_qsize 28, out_qsize 3\n",
      "2023-06-27 11:39:45,720 : INFO : EPOCH 8 - PROGRESS: at 82.33% examples, 286041 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:39:46,737 : INFO : EPOCH 8 - PROGRESS: at 89.88% examples, 290563 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:39:48,744 : INFO : EPOCH 8 - PROGRESS: at 96.10% examples, 273781 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:39:48,907 : INFO : EPOCH 8: training on 10976342 raw words (4863811 effective words) took 17.2s, 282031 effective words/s\n",
      "2023-06-27 11:39:49,930 : INFO : EPOCH 9 - PROGRESS: at 4.57% examples, 218637 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:39:50,951 : INFO : EPOCH 9 - PROGRESS: at 11.90% examples, 286930 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:39:51,978 : INFO : EPOCH 9 - PROGRESS: at 19.42% examples, 312387 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:39:52,990 : INFO : EPOCH 9 - PROGRESS: at 26.71% examples, 323889 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:39:54,983 : INFO : EPOCH 9 - PROGRESS: at 32.94% examples, 268363 words/s, in_qsize 22, out_qsize 6\n",
      "2023-06-27 11:39:55,983 : INFO : EPOCH 9 - PROGRESS: at 40.18% examples, 281753 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:39:56,988 : INFO : EPOCH 9 - PROGRESS: at 47.25% examples, 290448 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:39:58,011 : INFO : EPOCH 9 - PROGRESS: at 54.65% examples, 297373 words/s, in_qsize 28, out_qsize 2\n",
      "2023-06-27 11:39:59,022 : INFO : EPOCH 9 - PROGRESS: at 62.38% examples, 303962 words/s, in_qsize 27, out_qsize 2\n",
      "2023-06-27 11:40:01,045 : INFO : EPOCH 9 - PROGRESS: at 69.02% examples, 279044 words/s, in_qsize 25, out_qsize 3\n",
      "2023-06-27 11:40:02,049 : INFO : EPOCH 9 - PROGRESS: at 76.50% examples, 284950 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:40:03,088 : INFO : EPOCH 9 - PROGRESS: at 84.07% examples, 289741 words/s, in_qsize 27, out_qsize 1\n",
      "2023-06-27 11:40:04,088 : INFO : EPOCH 9 - PROGRESS: at 91.73% examples, 294516 words/s, in_qsize 27, out_qsize 1\n",
      "2023-06-27 11:40:04,891 : INFO : EPOCH 9: training on 10976342 raw words (4863162 effective words) took 16.0s, 304464 effective words/s\n",
      "2023-06-27 11:40:05,944 : INFO : EPOCH 10 - PROGRESS: at 4.84% examples, 231927 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:40:07,414 : INFO : EPOCH 10 - PROGRESS: at 6.54% examples, 128523 words/s, in_qsize 28, out_qsize 3\n",
      "2023-06-27 11:40:08,429 : INFO : EPOCH 10 - PROGRESS: at 13.68% examples, 192290 words/s, in_qsize 27, out_qsize 2\n",
      "2023-06-27 11:40:09,479 : INFO : EPOCH 10 - PROGRESS: at 20.84% examples, 226150 words/s, in_qsize 28, out_qsize 2\n",
      "2023-06-27 11:40:10,494 : INFO : EPOCH 10 - PROGRESS: at 28.05% examples, 249147 words/s, in_qsize 24, out_qsize 4\n",
      "2023-06-27 11:40:11,515 : INFO : EPOCH 10 - PROGRESS: at 35.34% examples, 265649 words/s, in_qsize 27, out_qsize 1\n",
      "2023-06-27 11:40:12,517 : INFO : EPOCH 10 - PROGRESS: at 42.45% examples, 277858 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:40:13,720 : INFO : EPOCH 10 - PROGRESS: at 42.81% examples, 241818 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:40:14,723 : INFO : EPOCH 10 - PROGRESS: at 49.80% examples, 252733 words/s, in_qsize 24, out_qsize 3\n",
      "2023-06-27 11:40:15,739 : INFO : EPOCH 10 - PROGRESS: at 57.08% examples, 260984 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:40:16,794 : INFO : EPOCH 10 - PROGRESS: at 62.67% examples, 260200 words/s, in_qsize 28, out_qsize 3\n",
      "2023-06-27 11:40:17,910 : INFO : EPOCH 10 - PROGRESS: at 68.83% examples, 260232 words/s, in_qsize 25, out_qsize 13\n",
      "2023-06-27 11:40:18,949 : INFO : EPOCH 10 - PROGRESS: at 76.41% examples, 266757 words/s, in_qsize 22, out_qsize 5\n",
      "2023-06-27 11:40:20,849 : INFO : EPOCH 10 - PROGRESS: at 79.47% examples, 244079 words/s, in_qsize 26, out_qsize 4\n",
      "2023-06-27 11:40:21,855 : INFO : EPOCH 10 - PROGRESS: at 85.48% examples, 246603 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:40:22,870 : INFO : EPOCH 10 - PROGRESS: at 91.08% examples, 247411 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:40:23,872 : INFO : EPOCH 10 - PROGRESS: at 97.87% examples, 251426 words/s, in_qsize 22, out_qsize 2\n",
      "2023-06-27 11:40:23,979 : INFO : EPOCH 10: training on 10976342 raw words (4864644 effective words) took 19.0s, 255389 effective words/s\n",
      "2023-06-27 11:40:25,035 : INFO : EPOCH 11 - PROGRESS: at 3.12% examples, 147349 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:40:26,078 : INFO : EPOCH 11 - PROGRESS: at 8.87% examples, 209995 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:40:27,108 : INFO : EPOCH 11 - PROGRESS: at 14.32% examples, 227353 words/s, in_qsize 22, out_qsize 6\n",
      "2023-06-27 11:40:28,861 : INFO : EPOCH 11 - PROGRESS: at 16.74% examples, 169769 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:40:29,895 : INFO : EPOCH 11 - PROGRESS: at 23.59% examples, 198143 words/s, in_qsize 22, out_qsize 1\n",
      "2023-06-27 11:40:30,897 : INFO : EPOCH 11 - PROGRESS: at 29.75% examples, 213418 words/s, in_qsize 24, out_qsize 3\n",
      "2023-06-27 11:40:31,906 : INFO : EPOCH 11 - PROGRESS: at 35.78% examples, 224326 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:40:32,936 : INFO : EPOCH 11 - PROGRESS: at 41.93% examples, 233218 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:40:33,949 : INFO : EPOCH 11 - PROGRESS: at 48.40% examples, 241881 words/s, in_qsize 25, out_qsize 3\n",
      "2023-06-27 11:40:35,890 : INFO : EPOCH 11 - PROGRESS: at 53.45% examples, 222936 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:40:36,914 : INFO : EPOCH 11 - PROGRESS: at 60.72% examples, 232041 words/s, in_qsize 26, out_qsize 2\n",
      "2023-06-27 11:40:37,932 : INFO : EPOCH 11 - PROGRESS: at 67.80% examples, 239187 words/s, in_qsize 28, out_qsize 2\n",
      "2023-06-27 11:40:38,949 : INFO : EPOCH 11 - PROGRESS: at 75.01% examples, 245807 words/s, in_qsize 28, out_qsize 3\n",
      "2023-06-27 11:40:39,987 : INFO : EPOCH 11 - PROGRESS: at 82.51% examples, 252407 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:40:40,988 : INFO : EPOCH 11 - PROGRESS: at 89.30% examples, 256532 words/s, in_qsize 28, out_qsize 3\n",
      "2023-06-27 11:40:42,272 : INFO : EPOCH 11 - PROGRESS: at 89.87% examples, 239946 words/s, in_qsize 25, out_qsize 3\n",
      "2023-06-27 11:40:43,279 : INFO : EPOCH 11 - PROGRESS: at 97.50% examples, 246260 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:40:43,406 : INFO : EPOCH 11: training on 10976342 raw words (4863924 effective words) took 19.4s, 250838 effective words/s\n",
      "2023-06-27 11:40:44,444 : INFO : EPOCH 12 - PROGRESS: at 4.56% examples, 216616 words/s, in_qsize 27, out_qsize 2\n",
      "2023-06-27 11:40:45,457 : INFO : EPOCH 12 - PROGRESS: at 11.72% examples, 282340 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:40:46,469 : INFO : EPOCH 12 - PROGRESS: at 18.16% examples, 293234 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:40:47,510 : INFO : EPOCH 12 - PROGRESS: at 25.27% examples, 305261 words/s, in_qsize 27, out_qsize 2\n",
      "2023-06-27 11:40:49,015 : INFO : EPOCH 12 - PROGRESS: at 27.07% examples, 238816 words/s, in_qsize 27, out_qsize 4\n",
      "2023-06-27 11:40:50,016 : INFO : EPOCH 12 - PROGRESS: at 33.93% examples, 254162 words/s, in_qsize 24, out_qsize 2\n",
      "2023-06-27 11:40:51,023 : INFO : EPOCH 12 - PROGRESS: at 39.74% examples, 259036 words/s, in_qsize 28, out_qsize 2\n",
      "2023-06-27 11:40:52,033 : INFO : EPOCH 12 - PROGRESS: at 45.65% examples, 262959 words/s, in_qsize 23, out_qsize 3\n",
      "2023-06-27 11:40:53,052 : INFO : EPOCH 12 - PROGRESS: at 51.25% examples, 264251 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:40:54,076 : INFO : EPOCH 12 - PROGRESS: at 58.11% examples, 269178 words/s, in_qsize 25, out_qsize 0\n",
      "2023-06-27 11:40:56,048 : INFO : EPOCH 12 - PROGRESS: at 63.31% examples, 246750 words/s, in_qsize 27, out_qsize 1\n",
      "2023-06-27 11:40:57,092 : INFO : EPOCH 12 - PROGRESS: at 71.00% examples, 254481 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:40:58,096 : INFO : EPOCH 12 - PROGRESS: at 78.35% examples, 261033 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:40:59,116 : INFO : EPOCH 12 - PROGRESS: at 85.77% examples, 266698 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:41:00,138 : INFO : EPOCH 12 - PROGRESS: at 93.04% examples, 271062 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:41:00,736 : INFO : EPOCH 12: training on 10976342 raw words (4864822 effective words) took 17.3s, 280951 effective words/s\n",
      "2023-06-27 11:41:02,460 : INFO : EPOCH 13 - PROGRESS: at 0.28% examples, 7972 words/s, in_qsize 24, out_qsize 3\n",
      "2023-06-27 11:41:03,472 : INFO : EPOCH 13 - PROGRESS: at 7.35% examples, 131902 words/s, in_qsize 27, out_qsize 2\n",
      "2023-06-27 11:41:04,488 : INFO : EPOCH 13 - PROGRESS: at 14.77% examples, 194133 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:41:05,508 : INFO : EPOCH 13 - PROGRESS: at 21.55% examples, 223568 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:41:06,532 : INFO : EPOCH 13 - PROGRESS: at 28.32% examples, 241939 words/s, in_qsize 26, out_qsize 5\n",
      "2023-06-27 11:41:07,606 : INFO : EPOCH 13 - PROGRESS: at 35.43% examples, 255645 words/s, in_qsize 28, out_qsize 2\n",
      "2023-06-27 11:41:08,888 : INFO : EPOCH 13 - PROGRESS: at 36.22% examples, 220290 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:41:09,901 : INFO : EPOCH 13 - PROGRESS: at 42.90% examples, 232635 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:41:10,974 : INFO : EPOCH 13 - PROGRESS: at 49.89% examples, 242367 words/s, in_qsize 27, out_qsize 4\n",
      "2023-06-27 11:41:12,001 : INFO : EPOCH 13 - PROGRESS: at 57.46% examples, 252212 words/s, in_qsize 23, out_qsize 5\n",
      "2023-06-27 11:41:13,012 : INFO : EPOCH 13 - PROGRESS: at 64.52% examples, 258854 words/s, in_qsize 26, out_qsize 3\n",
      "2023-06-27 11:41:14,024 : INFO : EPOCH 13 - PROGRESS: at 72.12% examples, 266216 words/s, in_qsize 27, out_qsize 2\n",
      "2023-06-27 11:41:15,225 : INFO : EPOCH 13 - PROGRESS: at 72.40% examples, 245029 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:41:16,248 : INFO : EPOCH 13 - PROGRESS: at 79.47% examples, 250669 words/s, in_qsize 28, out_qsize 2\n",
      "2023-06-27 11:41:17,270 : INFO : EPOCH 13 - PROGRESS: at 86.42% examples, 255294 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:41:18,270 : INFO : EPOCH 13 - PROGRESS: at 93.41% examples, 259707 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:41:18,801 : INFO : EPOCH 13: training on 10976342 raw words (4862587 effective words) took 18.0s, 269527 effective words/s\n",
      "2023-06-27 11:41:19,831 : INFO : EPOCH 14 - PROGRESS: at 4.29% examples, 205218 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:41:21,705 : INFO : EPOCH 14 - PROGRESS: at 9.31% examples, 157743 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:41:22,712 : INFO : EPOCH 14 - PROGRESS: at 16.20% examples, 204247 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:41:23,760 : INFO : EPOCH 14 - PROGRESS: at 22.87% examples, 228520 words/s, in_qsize 27, out_qsize 3\n",
      "2023-06-27 11:41:24,784 : INFO : EPOCH 14 - PROGRESS: at 30.37% examples, 251288 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:41:25,789 : INFO : EPOCH 14 - PROGRESS: at 36.92% examples, 262160 words/s, in_qsize 27, out_qsize 1\n",
      "2023-06-27 11:41:26,789 : INFO : EPOCH 14 - PROGRESS: at 43.17% examples, 268644 words/s, in_qsize 28, out_qsize 2\n",
      "2023-06-27 11:41:28,386 : INFO : EPOCH 14 - PROGRESS: at 46.01% examples, 238499 words/s, in_qsize 28, out_qsize 2\n",
      "2023-06-27 11:41:29,416 : INFO : EPOCH 14 - PROGRESS: at 53.17% examples, 248445 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:41:30,419 : INFO : EPOCH 14 - PROGRESS: at 60.44% examples, 256773 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:41:31,419 : INFO : EPOCH 14 - PROGRESS: at 67.62% examples, 263378 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:41:32,436 : INFO : EPOCH 14 - PROGRESS: at 75.10% examples, 269834 words/s, in_qsize 27, out_qsize 2\n",
      "2023-06-27 11:41:34,615 : INFO : EPOCH 14 - PROGRESS: at 82.51% examples, 255143 words/s, in_qsize 23, out_qsize 4\n",
      "2023-06-27 11:41:35,639 : INFO : EPOCH 14 - PROGRESS: at 89.88% examples, 260446 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:41:36,644 : INFO : EPOCH 14 - PROGRESS: at 98.89% examples, 269892 words/s, in_qsize 11, out_qsize 5\n",
      "2023-06-27 11:41:36,672 : INFO : EPOCH 14: training on 10976342 raw words (4865610 effective words) took 17.9s, 272470 effective words/s\n",
      "2023-06-27 11:41:37,709 : INFO : EPOCH 15 - PROGRESS: at 4.57% examples, 216437 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:41:38,723 : INFO : EPOCH 15 - PROGRESS: at 11.45% examples, 275891 words/s, in_qsize 24, out_qsize 4\n",
      "2023-06-27 11:41:39,746 : INFO : EPOCH 15 - PROGRESS: at 18.70% examples, 301042 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:41:41,060 : INFO : EPOCH 15 - PROGRESS: at 19.60% examples, 220719 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:41:42,069 : INFO : EPOCH 15 - PROGRESS: at 26.62% examples, 244262 words/s, in_qsize 26, out_qsize 2\n",
      "2023-06-27 11:41:43,084 : INFO : EPOCH 15 - PROGRESS: at 33.39% examples, 258057 words/s, in_qsize 23, out_qsize 4\n",
      "2023-06-27 11:41:44,098 : INFO : EPOCH 15 - PROGRESS: at 40.36% examples, 269965 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:41:45,112 : INFO : EPOCH 15 - PROGRESS: at 46.72% examples, 275187 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:41:46,146 : INFO : EPOCH 15 - PROGRESS: at 54.10% examples, 283190 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:41:47,718 : INFO : EPOCH 15 - PROGRESS: at 56.15% examples, 251656 words/s, in_qsize 23, out_qsize 5\n",
      "2023-06-27 11:41:48,760 : INFO : EPOCH 15 - PROGRESS: at 63.77% examples, 259964 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:41:49,780 : INFO : EPOCH 15 - PROGRESS: at 71.37% examples, 267158 words/s, in_qsize 24, out_qsize 4\n",
      "2023-06-27 11:41:50,799 : INFO : EPOCH 15 - PROGRESS: at 78.92% examples, 273405 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:41:51,812 : INFO : EPOCH 15 - PROGRESS: at 86.23% examples, 278209 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:41:54,176 : INFO : EPOCH 15 - PROGRESS: at 93.60% examples, 260636 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:41:54,663 : INFO : EPOCH 15: training on 10976342 raw words (4864523 effective words) took 18.0s, 270627 effective words/s\n",
      "2023-06-27 11:41:55,687 : INFO : EPOCH 16 - PROGRESS: at 4.29% examples, 207105 words/s, in_qsize 28, out_qsize 4\n",
      "2023-06-27 11:41:56,696 : INFO : EPOCH 16 - PROGRESS: at 11.90% examples, 289545 words/s, in_qsize 27, out_qsize 1\n",
      "2023-06-27 11:41:57,713 : INFO : EPOCH 16 - PROGRESS: at 19.15% examples, 311169 words/s, in_qsize 23, out_qsize 4\n",
      "2023-06-27 11:41:58,727 : INFO : EPOCH 16 - PROGRESS: at 25.91% examples, 316330 words/s, in_qsize 27, out_qsize 3\n",
      "2023-06-27 11:42:00,436 : INFO : EPOCH 16 - PROGRESS: at 30.02% examples, 257546 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:42:01,464 : INFO : EPOCH 16 - PROGRESS: at 37.27% examples, 272187 words/s, in_qsize 26, out_qsize 3\n",
      "2023-06-27 11:42:02,471 : INFO : EPOCH 16 - PROGRESS: at 44.58% examples, 283938 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:42:03,498 : INFO : EPOCH 16 - PROGRESS: at 51.64% examples, 290615 words/s, in_qsize 28, out_qsize 4\n",
      "2023-06-27 11:42:04,522 : INFO : EPOCH 16 - PROGRESS: at 59.22% examples, 296903 words/s, in_qsize 21, out_qsize 6\n",
      "2023-06-27 11:42:06,677 : INFO : EPOCH 16 - PROGRESS: at 66.00% examples, 270466 words/s, in_qsize 26, out_qsize 5\n",
      "2023-06-27 11:42:07,698 : INFO : EPOCH 16 - PROGRESS: at 73.23% examples, 275477 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:42:08,733 : INFO : EPOCH 16 - PROGRESS: at 80.67% examples, 280552 words/s, in_qsize 27, out_qsize 1\n",
      "2023-06-27 11:42:09,764 : INFO : EPOCH 16 - PROGRESS: at 87.61% examples, 283390 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:42:10,773 : INFO : EPOCH 16 - PROGRESS: at 94.63% examples, 286309 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:42:11,153 : INFO : EPOCH 16: training on 10976342 raw words (4865054 effective words) took 16.5s, 295382 effective words/s\n",
      "2023-06-27 11:42:13,130 : INFO : EPOCH 17 - PROGRESS: at 3.39% examples, 84119 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:42:14,143 : INFO : EPOCH 17 - PROGRESS: at 10.64% examples, 176185 words/s, in_qsize 27, out_qsize 2\n",
      "2023-06-27 11:42:15,169 : INFO : EPOCH 17 - PROGRESS: at 18.25% examples, 225221 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:42:16,185 : INFO : EPOCH 17 - PROGRESS: at 25.64% examples, 253015 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:42:17,185 : INFO : EPOCH 17 - PROGRESS: at 31.98% examples, 263007 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:42:18,191 : INFO : EPOCH 17 - PROGRESS: at 38.77% examples, 274008 words/s, in_qsize 24, out_qsize 3\n",
      "2023-06-27 11:42:19,589 : INFO : EPOCH 17 - PROGRESS: at 40.45% examples, 238474 words/s, in_qsize 25, out_qsize 0\n",
      "2023-06-27 11:42:20,615 : INFO : EPOCH 17 - PROGRESS: at 47.16% examples, 248072 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:42:21,652 : INFO : EPOCH 17 - PROGRESS: at 54.65% examples, 258298 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:42:22,657 : INFO : EPOCH 17 - PROGRESS: at 61.75% examples, 264998 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:42:23,691 : INFO : EPOCH 17 - PROGRESS: at 69.39% examples, 272001 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:42:25,801 : INFO : EPOCH 17 - PROGRESS: at 76.31% examples, 255339 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:42:26,844 : INFO : EPOCH 17 - PROGRESS: at 83.43% examples, 260090 words/s, in_qsize 28, out_qsize 3\n",
      "2023-06-27 11:42:27,882 : INFO : EPOCH 17 - PROGRESS: at 91.73% examples, 267509 words/s, in_qsize 23, out_qsize 0\n",
      "2023-06-27 11:42:28,692 : INFO : EPOCH 17: training on 10976342 raw words (4862554 effective words) took 17.5s, 277683 effective words/s\n",
      "2023-06-27 11:42:29,715 : INFO : EPOCH 18 - PROGRESS: at 4.57% examples, 220385 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:42:30,744 : INFO : EPOCH 18 - PROGRESS: at 11.90% examples, 287003 words/s, in_qsize 24, out_qsize 3\n",
      "2023-06-27 11:42:32,157 : INFO : EPOCH 18 - PROGRESS: at 13.86% examples, 197569 words/s, in_qsize 27, out_qsize 1\n",
      "2023-06-27 11:42:33,157 : INFO : EPOCH 18 - PROGRESS: at 20.84% examples, 231060 words/s, in_qsize 22, out_qsize 5\n",
      "2023-06-27 11:42:34,171 : INFO : EPOCH 18 - PROGRESS: at 28.14% examples, 254569 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:42:35,175 : INFO : EPOCH 18 - PROGRESS: at 35.43% examples, 271095 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:42:36,224 : INFO : EPOCH 18 - PROGRESS: at 42.73% examples, 282083 words/s, in_qsize 27, out_qsize 2\n",
      "2023-06-27 11:42:37,224 : INFO : EPOCH 18 - PROGRESS: at 49.89% examples, 291007 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:42:38,454 : INFO : EPOCH 18 - PROGRESS: at 49.98% examples, 254767 words/s, in_qsize 27, out_qsize 5\n",
      "2023-06-27 11:42:39,485 : INFO : EPOCH 18 - PROGRESS: at 57.93% examples, 265376 words/s, in_qsize 24, out_qsize 5\n",
      "2023-06-27 11:42:40,504 : INFO : EPOCH 18 - PROGRESS: at 65.81% examples, 274293 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:42:41,526 : INFO : EPOCH 18 - PROGRESS: at 73.42% examples, 280497 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:42:42,528 : INFO : EPOCH 18 - PROGRESS: at 80.86% examples, 285933 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:42:44,608 : INFO : EPOCH 18 - PROGRESS: at 87.61% examples, 268842 words/s, in_qsize 27, out_qsize 1\n",
      "2023-06-27 11:42:45,611 : INFO : EPOCH 18 - PROGRESS: at 94.90% examples, 273408 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:42:45,938 : INFO : EPOCH 18: training on 10976342 raw words (4864837 effective words) took 17.2s, 282404 effective words/s\n",
      "2023-06-27 11:42:46,979 : INFO : EPOCH 19 - PROGRESS: at 4.57% examples, 215832 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:42:47,984 : INFO : EPOCH 19 - PROGRESS: at 11.26% examples, 272252 words/s, in_qsize 25, out_qsize 4\n",
      "2023-06-27 11:42:49,015 : INFO : EPOCH 19 - PROGRESS: at 17.54% examples, 281751 words/s, in_qsize 21, out_qsize 10\n",
      "2023-06-27 11:42:50,029 : INFO : EPOCH 19 - PROGRESS: at 25.01% examples, 303237 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:42:51,201 : INFO : EPOCH 19 - PROGRESS: at 25.10% examples, 236382 words/s, in_qsize 19, out_qsize 11\n",
      "2023-06-27 11:42:52,207 : INFO : EPOCH 19 - PROGRESS: at 31.98% examples, 252680 words/s, in_qsize 24, out_qsize 3\n",
      "2023-06-27 11:42:53,222 : INFO : EPOCH 19 - PROGRESS: at 38.59% examples, 263143 words/s, in_qsize 24, out_qsize 3\n",
      "2023-06-27 11:42:54,238 : INFO : EPOCH 19 - PROGRESS: at 45.12% examples, 270343 words/s, in_qsize 25, out_qsize 4\n",
      "2023-06-27 11:42:55,291 : INFO : EPOCH 19 - PROGRESS: at 52.49% examples, 278864 words/s, in_qsize 24, out_qsize 4\n",
      "2023-06-27 11:42:56,342 : INFO : EPOCH 19 - PROGRESS: at 59.69% examples, 283473 words/s, in_qsize 27, out_qsize 4\n",
      "2023-06-27 11:42:57,801 : INFO : EPOCH 19 - PROGRESS: at 61.56% examples, 256041 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:42:58,805 : INFO : EPOCH 19 - PROGRESS: at 68.09% examples, 260069 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:42:59,825 : INFO : EPOCH 19 - PROGRESS: at 75.67% examples, 266963 words/s, in_qsize 25, out_qsize 3\n",
      "2023-06-27 11:43:00,829 : INFO : EPOCH 19 - PROGRESS: at 82.97% examples, 272491 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:43:01,886 : INFO : EPOCH 19 - PROGRESS: at 90.07% examples, 275516 words/s, in_qsize 27, out_qsize 6\n",
      "2023-06-27 11:43:02,800 : INFO : EPOCH 19: training on 10976342 raw words (4864393 effective words) took 16.8s, 288750 effective words/s\n",
      "2023-06-27 11:43:04,364 : INFO : EPOCH 20 - PROGRESS: at 0.19% examples, 5792 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:43:05,385 : INFO : EPOCH 20 - PROGRESS: at 7.80% examples, 148574 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:43:06,407 : INFO : EPOCH 20 - PROGRESS: at 15.21% examples, 208531 words/s, in_qsize 22, out_qsize 1\n",
      "2023-06-27 11:43:07,489 : INFO : EPOCH 20 - PROGRESS: at 22.69% examples, 240418 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:43:08,568 : INFO : EPOCH 20 - PROGRESS: at 30.20% examples, 259607 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:43:10,564 : INFO : EPOCH 20 - PROGRESS: at 34.99% examples, 223604 words/s, in_qsize 27, out_qsize 1\n",
      "2023-06-27 11:43:11,571 : INFO : EPOCH 20 - PROGRESS: at 40.97% examples, 232255 words/s, in_qsize 21, out_qsize 1\n",
      "2023-06-27 11:43:12,590 : INFO : EPOCH 20 - PROGRESS: at 47.51% examples, 241480 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:43:13,614 : INFO : EPOCH 20 - PROGRESS: at 54.47% examples, 249886 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:43:14,655 : INFO : EPOCH 20 - PROGRESS: at 60.81% examples, 253301 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:43:15,732 : INFO : EPOCH 20 - PROGRESS: at 67.99% examples, 258539 words/s, in_qsize 21, out_qsize 6\n",
      "2023-06-27 11:43:17,377 : INFO : EPOCH 20 - PROGRESS: at 71.27% examples, 240000 words/s, in_qsize 24, out_qsize 3\n",
      "2023-06-27 11:43:18,410 : INFO : EPOCH 20 - PROGRESS: at 78.35% examples, 245790 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:43:19,431 : INFO : EPOCH 20 - PROGRESS: at 85.48% examples, 251243 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:43:20,434 : INFO : EPOCH 20 - PROGRESS: at 91.36% examples, 252721 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:43:21,373 : INFO : EPOCH 20: training on 10976342 raw words (4863378 effective words) took 18.5s, 262234 effective words/s\n",
      "2023-06-27 11:43:22,398 : INFO : EPOCH 21 - PROGRESS: at 4.21% examples, 202863 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:43:24,397 : INFO : EPOCH 21 - PROGRESS: at 8.60% examples, 139887 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:43:25,397 : INFO : EPOCH 21 - PROGRESS: at 13.86% examples, 170079 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:43:26,433 : INFO : EPOCH 21 - PROGRESS: at 19.86% examples, 194294 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:43:27,506 : INFO : EPOCH 21 - PROGRESS: at 26.26% examples, 212378 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:43:28,516 : INFO : EPOCH 21 - PROGRESS: at 33.12% examples, 229938 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:43:29,560 : INFO : EPOCH 21 - PROGRESS: at 39.48% examples, 239649 words/s, in_qsize 26, out_qsize 2\n",
      "2023-06-27 11:43:31,631 : INFO : EPOCH 21 - PROGRESS: at 44.76% examples, 217024 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:43:32,639 : INFO : EPOCH 21 - PROGRESS: at 49.11% examples, 216791 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:43:33,655 : INFO : EPOCH 21 - PROGRESS: at 56.52% examples, 227802 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:43:34,665 : INFO : EPOCH 21 - PROGRESS: at 63.96% examples, 237140 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:43:35,678 : INFO : EPOCH 21 - PROGRESS: at 71.18% examples, 244187 words/s, in_qsize 23, out_qsize 4\n",
      "2023-06-27 11:43:36,707 : INFO : EPOCH 21 - PROGRESS: at 79.01% examples, 252178 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:43:38,170 : INFO : EPOCH 21 - PROGRESS: at 81.50% examples, 237307 words/s, in_qsize 26, out_qsize 0\n",
      "2023-06-27 11:43:39,199 : INFO : EPOCH 21 - PROGRESS: at 88.64% examples, 242752 words/s, in_qsize 26, out_qsize 5\n",
      "2023-06-27 11:43:40,199 : INFO : EPOCH 21 - PROGRESS: at 96.47% examples, 249615 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:43:40,344 : INFO : EPOCH 21: training on 10976342 raw words (4863959 effective words) took 19.0s, 256661 effective words/s\n",
      "2023-06-27 11:43:41,438 : INFO : EPOCH 22 - PROGRESS: at 4.75% examples, 212780 words/s, in_qsize 21, out_qsize 6\n",
      "2023-06-27 11:43:42,453 : INFO : EPOCH 22 - PROGRESS: at 11.18% examples, 261909 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:43:44,766 : INFO : EPOCH 22 - PROGRESS: at 18.17% examples, 202844 words/s, in_qsize 28, out_qsize 4\n",
      "2023-06-27 11:43:45,784 : INFO : EPOCH 22 - PROGRESS: at 25.10% examples, 228561 words/s, in_qsize 24, out_qsize 3\n",
      "2023-06-27 11:43:46,806 : INFO : EPOCH 22 - PROGRESS: at 31.89% examples, 244296 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:43:47,820 : INFO : EPOCH 22 - PROGRESS: at 38.42% examples, 255014 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:43:48,848 : INFO : EPOCH 22 - PROGRESS: at 45.56% examples, 266331 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:43:49,880 : INFO : EPOCH 22 - PROGRESS: at 52.88% examples, 275194 words/s, in_qsize 26, out_qsize 2\n",
      "2023-06-27 11:43:51,239 : INFO : EPOCH 22 - PROGRESS: at 54.47% examples, 247716 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:43:52,245 : INFO : EPOCH 22 - PROGRESS: at 61.10% examples, 253223 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:43:53,252 : INFO : EPOCH 22 - PROGRESS: at 68.37% examples, 260115 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:43:54,266 : INFO : EPOCH 22 - PROGRESS: at 75.66% examples, 266131 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:43:55,272 : INFO : EPOCH 22 - PROGRESS: at 82.15% examples, 269026 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:43:56,312 : INFO : EPOCH 22 - PROGRESS: at 88.93% examples, 271725 words/s, in_qsize 20, out_qsize 8\n",
      "2023-06-27 11:43:57,877 : INFO : EPOCH 22 - PROGRESS: at 91.17% examples, 253519 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:43:58,681 : INFO : EPOCH 22: training on 10976342 raw words (4863436 effective words) took 18.3s, 265440 effective words/s\n",
      "2023-06-27 11:43:59,728 : INFO : EPOCH 23 - PROGRESS: at 4.57% examples, 213501 words/s, in_qsize 28, out_qsize 2\n",
      "2023-06-27 11:44:00,737 : INFO : EPOCH 23 - PROGRESS: at 11.72% examples, 281103 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:44:01,795 : INFO : EPOCH 23 - PROGRESS: at 19.24% examples, 305668 words/s, in_qsize 26, out_qsize 4\n",
      "2023-06-27 11:44:02,799 : INFO : EPOCH 23 - PROGRESS: at 26.63% examples, 320272 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:44:04,156 : INFO : EPOCH 23 - PROGRESS: at 27.96% examples, 252764 words/s, in_qsize 28, out_qsize 3\n",
      "2023-06-27 11:44:05,228 : INFO : EPOCH 23 - PROGRESS: at 35.51% examples, 268914 words/s, in_qsize 25, out_qsize 6\n",
      "2023-06-27 11:44:06,232 : INFO : EPOCH 23 - PROGRESS: at 42.90% examples, 282375 words/s, in_qsize 23, out_qsize 4\n",
      "2023-06-27 11:44:07,255 : INFO : EPOCH 23 - PROGRESS: at 50.14% examples, 290928 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:44:08,277 : INFO : EPOCH 23 - PROGRESS: at 57.65% examples, 296870 words/s, in_qsize 23, out_qsize 4\n",
      "2023-06-27 11:44:10,375 : INFO : EPOCH 23 - PROGRESS: at 64.33% examples, 270879 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:44:11,382 : INFO : EPOCH 23 - PROGRESS: at 71.56% examples, 276273 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:44:12,410 : INFO : EPOCH 23 - PROGRESS: at 79.10% examples, 281838 words/s, in_qsize 23, out_qsize 3\n",
      "2023-06-27 11:44:13,435 : INFO : EPOCH 23 - PROGRESS: at 86.51% examples, 286247 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:44:14,449 : INFO : EPOCH 23 - PROGRESS: at 93.42% examples, 288656 words/s, in_qsize 27, out_qsize 5\n",
      "2023-06-27 11:44:14,919 : INFO : EPOCH 23: training on 10976342 raw words (4862738 effective words) took 16.2s, 299700 effective words/s\n",
      "2023-06-27 11:44:16,714 : INFO : EPOCH 24 - PROGRESS: at 1.20% examples, 33837 words/s, in_qsize 22, out_qsize 2\n",
      "2023-06-27 11:44:17,756 : INFO : EPOCH 24 - PROGRESS: at 8.25% examples, 145534 words/s, in_qsize 27, out_qsize 1\n",
      "2023-06-27 11:44:18,762 : INFO : EPOCH 24 - PROGRESS: at 15.39% examples, 200403 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:44:19,811 : INFO : EPOCH 24 - PROGRESS: at 22.52% examples, 230762 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:44:20,824 : INFO : EPOCH 24 - PROGRESS: at 29.58% examples, 250298 words/s, in_qsize 26, out_qsize 3\n",
      "2023-06-27 11:44:21,838 : INFO : EPOCH 24 - PROGRESS: at 36.39% examples, 263171 words/s, in_qsize 21, out_qsize 6\n",
      "2023-06-27 11:44:23,217 : INFO : EPOCH 24 - PROGRESS: at 37.89% examples, 228159 words/s, in_qsize 22, out_qsize 2\n",
      "2023-06-27 11:44:24,233 : INFO : EPOCH 24 - PROGRESS: at 44.13% examples, 237080 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:44:25,317 : INFO : EPOCH 24 - PROGRESS: at 51.54% examples, 247855 words/s, in_qsize 22, out_qsize 5\n",
      "2023-06-27 11:44:26,377 : INFO : EPOCH 24 - PROGRESS: at 58.86% examples, 255094 words/s, in_qsize 26, out_qsize 3\n",
      "2023-06-27 11:44:27,400 : INFO : EPOCH 24 - PROGRESS: at 65.73% examples, 260440 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:44:28,405 : INFO : EPOCH 24 - PROGRESS: at 72.31% examples, 264131 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:44:29,988 : INFO : EPOCH 24 - PROGRESS: at 74.07% examples, 241837 words/s, in_qsize 24, out_qsize 1\n",
      "2023-06-27 11:44:31,118 : INFO : EPOCH 24 - PROGRESS: at 80.95% examples, 245293 words/s, in_qsize 21, out_qsize 8\n",
      "2023-06-27 11:44:32,138 : INFO : EPOCH 24 - PROGRESS: at 88.08% examples, 250554 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:44:33,210 : INFO : EPOCH 24 - PROGRESS: at 95.08% examples, 254081 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:44:33,571 : INFO : EPOCH 24: training on 10976342 raw words (4864085 effective words) took 18.6s, 261824 effective words/s\n",
      "2023-06-27 11:44:34,609 : INFO : EPOCH 25 - PROGRESS: at 3.93% examples, 187698 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:44:35,621 : INFO : EPOCH 25 - PROGRESS: at 10.73% examples, 259382 words/s, in_qsize 25, out_qsize 3\n",
      "2023-06-27 11:44:36,880 : INFO : EPOCH 25 - PROGRESS: at 10.82% examples, 161385 words/s, in_qsize 28, out_qsize 4\n",
      "2023-06-27 11:44:37,883 : INFO : EPOCH 25 - PROGRESS: at 17.72% examples, 203252 words/s, in_qsize 23, out_qsize 4\n",
      "2023-06-27 11:44:38,921 : INFO : EPOCH 25 - PROGRESS: at 24.92% examples, 231016 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:44:39,939 : INFO : EPOCH 25 - PROGRESS: at 32.25% examples, 250997 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:44:40,939 : INFO : EPOCH 25 - PROGRESS: at 39.21% examples, 264525 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:44:41,960 : INFO : EPOCH 25 - PROGRESS: at 46.37% examples, 274904 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:44:43,345 : INFO : EPOCH 25 - PROGRESS: at 47.69% examples, 242687 words/s, in_qsize 23, out_qsize 5\n",
      "2023-06-27 11:44:44,400 : INFO : EPOCH 25 - PROGRESS: at 54.84% examples, 251092 words/s, in_qsize 27, out_qsize 4\n",
      "2023-06-27 11:44:45,406 : INFO : EPOCH 25 - PROGRESS: at 62.39% examples, 260098 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:44:46,456 : INFO : EPOCH 25 - PROGRESS: at 69.95% examples, 266622 words/s, in_qsize 26, out_qsize 3\n",
      "2023-06-27 11:44:47,472 : INFO : EPOCH 25 - PROGRESS: at 77.42% examples, 272777 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:44:49,590 : INFO : EPOCH 25 - PROGRESS: at 84.08% examples, 256595 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:44:50,598 : INFO : EPOCH 25 - PROGRESS: at 90.99% examples, 260636 words/s, in_qsize 28, out_qsize 3\n",
      "2023-06-27 11:44:51,437 : INFO : EPOCH 25: training on 10976342 raw words (4862921 effective words) took 17.8s, 272541 effective words/s\n",
      "2023-06-27 11:44:52,528 : INFO : EPOCH 26 - PROGRESS: at 4.84% examples, 221836 words/s, in_qsize 27, out_qsize 1\n",
      "2023-06-27 11:44:53,552 : INFO : EPOCH 26 - PROGRESS: at 11.81% examples, 278490 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:44:54,554 : INFO : EPOCH 26 - PROGRESS: at 18.70% examples, 298658 words/s, in_qsize 27, out_qsize 3\n",
      "2023-06-27 11:44:56,030 : INFO : EPOCH 26 - PROGRESS: at 21.02% examples, 227229 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:44:57,041 : INFO : EPOCH 26 - PROGRESS: at 27.78% examples, 246154 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:44:58,043 : INFO : EPOCH 26 - PROGRESS: at 34.37% examples, 258444 words/s, in_qsize 21, out_qsize 6\n",
      "2023-06-27 11:44:59,061 : INFO : EPOCH 26 - PROGRESS: at 41.57% examples, 271726 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:45:00,081 : INFO : EPOCH 26 - PROGRESS: at 48.40% examples, 279003 words/s, in_qsize 27, out_qsize 1\n",
      "2023-06-27 11:45:01,099 : INFO : EPOCH 26 - PROGRESS: at 55.68% examples, 285976 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:45:02,450 : INFO : EPOCH 26 - PROGRESS: at 57.18% examples, 257190 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:45:03,515 : INFO : EPOCH 26 - PROGRESS: at 64.42% examples, 263185 words/s, in_qsize 27, out_qsize 1\n",
      "2023-06-27 11:45:04,532 : INFO : EPOCH 26 - PROGRESS: at 71.47% examples, 268140 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:45:05,538 : INFO : EPOCH 26 - PROGRESS: at 78.82% examples, 273938 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:45:06,550 : INFO : EPOCH 26 - PROGRESS: at 86.42% examples, 279665 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:45:07,572 : INFO : EPOCH 26 - PROGRESS: at 93.60% examples, 283116 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:45:08,801 : INFO : EPOCH 26 - PROGRESS: at 93.70% examples, 263293 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:45:09,328 : INFO : EPOCH 26: training on 10976342 raw words (4863823 effective words) took 17.9s, 272405 effective words/s\n",
      "2023-06-27 11:45:10,379 : INFO : EPOCH 27 - PROGRESS: at 4.57% examples, 214882 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:45:11,409 : INFO : EPOCH 27 - PROGRESS: at 11.72% examples, 279249 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:45:12,454 : INFO : EPOCH 27 - PROGRESS: at 18.70% examples, 296759 words/s, in_qsize 27, out_qsize 3\n",
      "2023-06-27 11:45:13,578 : INFO : EPOCH 27 - PROGRESS: at 26.53% examples, 309883 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:45:15,387 : INFO : EPOCH 27 - PROGRESS: at 31.27% examples, 255747 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:45:16,414 : INFO : EPOCH 27 - PROGRESS: at 38.33% examples, 268798 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:45:17,432 : INFO : EPOCH 27 - PROGRESS: at 45.12% examples, 277049 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:45:18,562 : INFO : EPOCH 27 - PROGRESS: at 51.93% examples, 279677 words/s, in_qsize 24, out_qsize 3\n",
      "2023-06-27 11:45:19,599 : INFO : EPOCH 27 - PROGRESS: at 58.95% examples, 283792 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:45:20,620 : INFO : EPOCH 27 - PROGRESS: at 65.35% examples, 285166 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:45:22,222 : INFO : EPOCH 27 - PROGRESS: at 67.90% examples, 258953 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:45:23,221 : INFO : EPOCH 27 - PROGRESS: at 75.01% examples, 264631 words/s, in_qsize 27, out_qsize 1\n",
      "2023-06-27 11:45:24,233 : INFO : EPOCH 27 - PROGRESS: at 82.42% examples, 270562 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:45:25,275 : INFO : EPOCH 27 - PROGRESS: at 90.16% examples, 275950 words/s, in_qsize 24, out_qsize 0\n",
      "2023-06-27 11:45:26,284 : INFO : EPOCH 27 - PROGRESS: at 98.43% examples, 282781 words/s, in_qsize 17, out_qsize 1\n",
      "2023-06-27 11:45:26,327 : INFO : EPOCH 27: training on 10976342 raw words (4863758 effective words) took 17.0s, 286512 effective words/s\n",
      "2023-06-27 11:45:27,352 : INFO : EPOCH 28 - PROGRESS: at 4.39% examples, 211887 words/s, in_qsize 28, out_qsize 2\n",
      "2023-06-27 11:45:28,577 : INFO : EPOCH 28 - PROGRESS: at 4.48% examples, 97303 words/s, in_qsize 27, out_qsize 2\n",
      "2023-06-27 11:45:29,610 : INFO : EPOCH 28 - PROGRESS: at 11.54% examples, 173372 words/s, in_qsize 25, out_qsize 4\n",
      "2023-06-27 11:45:30,628 : INFO : EPOCH 28 - PROGRESS: at 18.98% examples, 218194 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:45:31,637 : INFO : EPOCH 28 - PROGRESS: at 25.63% examples, 239267 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:45:32,655 : INFO : EPOCH 28 - PROGRESS: at 31.89% examples, 249569 words/s, in_qsize 28, out_qsize 3\n",
      "2023-06-27 11:45:33,665 : INFO : EPOCH 28 - PROGRESS: at 38.60% examples, 261107 words/s, in_qsize 26, out_qsize 2\n",
      "2023-06-27 11:45:35,290 : INFO : EPOCH 28 - PROGRESS: at 41.92% examples, 232514 words/s, in_qsize 18, out_qsize 2\n",
      "2023-06-27 11:45:36,313 : INFO : EPOCH 28 - PROGRESS: at 48.57% examples, 241845 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:45:37,315 : INFO : EPOCH 28 - PROGRESS: at 55.49% examples, 250085 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:45:38,320 : INFO : EPOCH 28 - PROGRESS: at 62.75% examples, 257964 words/s, in_qsize 25, out_qsize 2\n",
      "2023-06-27 11:45:39,323 : INFO : EPOCH 28 - PROGRESS: at 69.95% examples, 264224 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:45:40,335 : INFO : EPOCH 28 - PROGRESS: at 77.14% examples, 269646 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:45:41,665 : INFO : EPOCH 28 - PROGRESS: at 77.79% examples, 248275 words/s, in_qsize 28, out_qsize 1\n",
      "2023-06-27 11:45:42,679 : INFO : EPOCH 28 - PROGRESS: at 84.54% examples, 252643 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:45:43,767 : INFO : EPOCH 28 - PROGRESS: at 91.08% examples, 254622 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:45:44,638 : INFO : EPOCH 28: training on 10976342 raw words (4861774 effective words) took 18.3s, 265815 effective words/s\n",
      "2023-06-27 11:45:45,716 : INFO : EPOCH 29 - PROGRESS: at 4.66% examples, 213204 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:45:46,738 : INFO : EPOCH 29 - PROGRESS: at 11.18% examples, 263588 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:45:48,531 : INFO : EPOCH 29 - PROGRESS: at 14.59% examples, 184876 words/s, in_qsize 25, out_qsize 3\n",
      "2023-06-27 11:45:49,589 : INFO : EPOCH 29 - PROGRESS: at 21.55% examples, 215564 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:45:50,594 : INFO : EPOCH 29 - PROGRESS: at 27.87% examples, 231841 words/s, in_qsize 22, out_qsize 5\n",
      "2023-06-27 11:45:51,620 : INFO : EPOCH 29 - PROGRESS: at 34.19% examples, 242740 words/s, in_qsize 26, out_qsize 4\n",
      "2023-06-27 11:45:52,627 : INFO : EPOCH 29 - PROGRESS: at 40.19% examples, 249863 words/s, in_qsize 26, out_qsize 1\n",
      "2023-06-27 11:45:53,670 : INFO : EPOCH 29 - PROGRESS: at 46.36% examples, 255168 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:45:55,615 : INFO : EPOCH 29 - PROGRESS: at 50.78% examples, 230227 words/s, in_qsize 25, out_qsize 4\n",
      "2023-06-27 11:45:56,616 : INFO : EPOCH 29 - PROGRESS: at 58.58% examples, 241686 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:45:57,645 : INFO : EPOCH 29 - PROGRESS: at 65.72% examples, 248665 words/s, in_qsize 27, out_qsize 3\n",
      "2023-06-27 11:45:58,657 : INFO : EPOCH 29 - PROGRESS: at 73.42% examples, 256668 words/s, in_qsize 27, out_qsize 1\n",
      "2023-06-27 11:45:59,702 : INFO : EPOCH 29 - PROGRESS: at 80.58% examples, 261625 words/s, in_qsize 25, out_qsize 3\n",
      "2023-06-27 11:46:01,830 : INFO : EPOCH 29 - PROGRESS: at 87.71% examples, 249044 words/s, in_qsize 28, out_qsize 0\n",
      "2023-06-27 11:46:02,831 : INFO : EPOCH 29 - PROGRESS: at 95.08% examples, 254602 words/s, in_qsize 27, out_qsize 0\n",
      "2023-06-27 11:46:03,161 : INFO : EPOCH 29: training on 10976342 raw words (4861644 effective words) took 18.5s, 262774 effective words/s\n",
      "2023-06-27 11:46:03,162 : INFO : Word2Vec lifecycle event {'msg': 'training on 329290260 raw words (145915192 effective words) took 532.2s, 274197 effective words/s', 'datetime': '2023-06-27T11:46:03.162093', 'gensim': '4.3.0', 'python': '3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "took time 532.1568157672882 s\n"
     ]
    }
   ],
   "source": [
    "t= time.time()\n",
    "# and now, train it. the important part\n",
    "# as expected, takes a bit of time. \n",
    "# more epochs will result in better results as a rule of thumb\n",
    "# but 30 is fine.\n",
    "# total examples \n",
    "genshinmodel.train(\n",
    "    sentences, # the corpus\n",
    "    total_examples=genshinmodel.corpus_count, # number of sentences\n",
    "    epochs=30, # how many epochs to train for\n",
    "    report_delay=1 # progress report how often, in seconds. can honestly set it to high 10s\n",
    "    )\n",
    "print(\"\\n\\ntook time\", (time.time()-t), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stemmer = SnowballStemmer('english') # stemmer for similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('exam', 0.5778243541717529),\n",
       " ('studi', 0.532958984375),\n",
       " ('math', 0.5091339945793152),\n",
       " ('fail', 0.47623127698898315),\n",
       " ('class', 0.4673853814601898),\n",
       " ('math_test', 0.46504122018814087),\n",
       " ('math_gcse', 0.4571109116077423),\n",
       " ('oral_exam', 0.4550704061985016),\n",
       " ('calc', 0.4524213969707489),\n",
       " ('scienc', 0.45091551542282104)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genshinmodel.wv.most_similar(stemmer.stem('test')) # show the similarity checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the inputs\n",
    "# initiliaze ont he dataset\n",
    "\n",
    "\n",
    "tknizer = Tokenizer()\n",
    "tknizer.fit_on_texts(df.Clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (1038058, 140)\n",
      "x_test (444883, 140)\n"
     ]
    }
   ],
   "source": [
    "# ensure they are of the same length by padding. check pad_sequences docu for various params\n",
    "# doing simple pre-padding here\n",
    "\n",
    "\n",
    "x_train = pad_sequences(tknizer.texts_to_sequences(train_data.Clean), maxlen=140)\n",
    "x_test = pad_sequences(tknizer.texts_to_sequences(test_data.Clean), maxlen=140) # twitter absolute max size\n",
    "\n",
    "print(\"x_train\", x_train.shape)\n",
    "print(\"x_test\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative', 'Positive']\n",
      "y_train (1038058, 1)\n",
      "y_test (444883, 1)\n"
     ]
    }
   ],
   "source": [
    "# encode labels and transform \n",
    "\n",
    "labels = df.target.unique().tolist()\n",
    "print(labels)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df.target.tolist()) # initialize encoder logic\n",
    "\n",
    "# the usual\n",
    "# x -> features\n",
    "# y -> labels\n",
    "# we encode the labels \n",
    "\n",
    "y_train = encoder.transform(train_data.target.tolist()) # encode the target into y_train\n",
    "y_test = encoder.transform(test_data.target.tolist()) # encode to y_test\n",
    "\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "print(\"y_train\",y_train.shape)\n",
    "print(\"y_test\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]] [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0 1279   44   91  278  283  100   42   52]\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "# did we reshape, did we matrixizce the inputs\n",
    "print(\n",
    "y_test[0:5],\n",
    "x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(454515, 300)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# embed the precious genshim word2vec model\n",
    "\n",
    "embedding_matrix = np.zeros((len(wrdfreq), 300))\n",
    "for word, i in tknizer.word_index.items():\n",
    "  if word in genshinmodel.wv:\n",
    "    embedding_matrix[i] = genshinmodel.wv[word]\n",
    "\n",
    "# the embedding matrix maps index to vectors\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Time to define model layers  \n",
    "lets start with our emdedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(wrdfreq), 300, weights=[embedding_matrix], input_length=140, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 140, 300)          136354500 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 140, 300)          0         \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 140, 200)          400800    \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 140, 100)          120400    \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 50)                30200     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 136,905,951\n",
      "Trainable params: 551,451\n",
      "Non-trainable params: 136,354,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# rest of the model layers\n",
    "model = Sequential()\n",
    "model.add(embedding_layer) # highest parameters, most important\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(200, dropout=0.2, return_sequences=True)) # multiple LSTM layers \n",
    "# dropouts prevent overfitting, and lower reliance on regularly occuring words\n",
    "# encourages to actually learn and not take shortcuts\n",
    "# afterall, its not a real human, we cant have it take shortcuts\n",
    "model.add(LSTM(100, recurrent_dropout=0.1, return_sequences=True)) \n",
    "# recurrent dropout drops the connection between recurernt cells\n",
    "# i.e; memory and hidden. captures longer term dependancies\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(1, activation='sigmoid')) # sigmoid because we need binary output\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback so we dont train forever\n",
    "# model checkpoint added retrospectively\n",
    "# now im stuck waiting for this to be finished, which will take forever. Sadge\n",
    "\n",
    "\n",
    "callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=3, cooldown=0),\n",
    "              EarlyStopping(monitor='val_acc', min_delta=2e-3, patience=3),\n",
    "               ModelCheckpoint(filepath='model_checkpoint.h5', save_best_only=True, save_weights_only=False)\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add logic to check for model checkpoint to resume training from\n",
    "# and alternatively also CHECK for models, so that \n",
    "# i dont have to train anyways, if one exists. \n",
    "# i'll just raise an error for now.\n",
    "# crude, but works. thats what countsp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1522/1825 [========================>.....] - ETA: 41:43 - loss: 0.4908 - accuracy: 0.7606"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> 2\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train, y_train,\n\u001b[0;32m      3\u001b[0m                     batch_size\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m,\n\u001b[0;32m      4\u001b[0m                     epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[0;32m      5\u001b[0m                     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[0;32m      6\u001b[0m                     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m      7\u001b[0m                     callbacks\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtime taken\u001b[39m\u001b[39m\"\u001b[39m, time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t, \u001b[39m\"\u001b[39m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "# train. my system takes ~ 3 hrs or so\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=512,\n",
    "                    epochs=3,\n",
    "                    validation_split=0.1,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "print(\"time taken\", time.time() - t, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple evaluation\n",
    "score = model.evaluate(x_test, y_test, batch_size=512)\n",
    "print(\"accuracy:\",score[1])\n",
    "print(\"loss:\",score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lets save the model because we dont want to train it for 2 hours every time we open the notebook\n",
    "model.save('snetiment_analzyer.h5')\n",
    "# lets zip it too, so we can save space\n",
    "with zipfile.ZipFile('snetiment_analzyer.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write('snetiment_analzyer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have a history object from earlier, we can plot how our model evolved\n",
    "# really helpful to see some cool outputs\n",
    "\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    " \n",
    "epochs = range(len(acc)) # 3 for us, so not real 'graph', just a few points\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets gor fo prediction now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/869 [==>...........................] - ETA: 11:22 - loss: 0.4503 - accuracy: 0.7881"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# prediction code dummy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# please don't forget to PREDICT, sincerely\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# myself\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# load the checkpoint\u001b[39;00m\n\u001b[0;32m      5\u001b[0m checkpoint \u001b[39m=\u001b[39m load_model(\u001b[39m'\u001b[39m\u001b[39mmodel_checkpoint.h5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m score \u001b[39m=\u001b[39m checkpoint\u001b[39m.\u001b[39;49mevaluate(x_test, y_test, batch_size\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39maccuracy:\u001b[39m\u001b[39m\"\u001b[39m,score[\u001b[39m1\u001b[39m])\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mloss:\u001b[39m\u001b[39m\"\u001b[39m,score[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py:2072\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2068\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   2069\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, step_num\u001b[39m=\u001b[39mstep, _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m   2070\u001b[0m ):\n\u001b[0;32m   2071\u001b[0m     callbacks\u001b[39m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 2072\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_function(iterator)\n\u001b[0;32m   2073\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   2074\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    931\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    934\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    935\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    936\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# prediction code dummy\n",
    "# please don't forget to PREDICT, sincerely\n",
    "# myself\n",
    "# load the checkpoint\n",
    "checkpoint = load_model('model_checkpoint.h5')\n",
    "score = checkpoint.evaluate(x_test, y_test, batch_size=512)\n",
    "print(\"accuracy:\",score[1])\n",
    "print(\"loss:\",score[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
