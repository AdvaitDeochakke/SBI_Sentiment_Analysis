{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "\n",
    "bigdf = pd.read_csv(\"bigasstweetsdataset.csv\", encoding=\"latin-1\") # this is in .gitignore, its the sentiment140 dataset on kaggle\n",
    "# linke -> https://www.kaggle.com/datasets/kazanova/sentiment140\n",
    "bigdf.columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"TweetText\"]\n",
    "bigdf.drop(['ids', 'flag', 'date', 'user'], axis=1, inplace=True) # since we are only detecting depressing sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>TweetText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                          TweetText\n",
       "0       0  is upset that he can't update his Facebook by ...\n",
       "1       0  @Kenichan I dived many times for the ball. Man...\n",
       "2       0    my whole body feels itchy and like its on fire \n",
       "3       0  @nationwideclass no, it's not behaving at all....\n",
       "4       0                      @Kwesidei not the whole crew "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The types of targets ->  [0 4]\n",
      "Given that 0 -> Negative Sentiment; 4 -> Positive Sentiment\n",
      "The counts are\n",
      "4    800000\n",
      "0    799999\n",
      "Name: target, dtype: int64\n",
      "Pretty balanced dataset, lets convert labels to categories, readable and clear\n"
     ]
    }
   ],
   "source": [
    "# label conversion and counting\n",
    "print(\"The types of targets -> \", bigdf['target'].unique())\n",
    "print(\"Given that 0 -> Negative Sentiment; 4 -> Positive Sentiment\")\n",
    "print(\"The counts are\\n\", bigdf['target'].value_counts(), sep=\"\")\n",
    "print(\"Pretty balanced dataset, lets convert labels to categories, readable and clear\")\n",
    "\n",
    "label_mapper = {4 : \"Positive\", 0 : \"Negative\"}\n",
    "bigdf['target'] = bigdf['target'].map(label_mapper)\n",
    "del label_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>TweetText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Negative</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Negative</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                          TweetText\n",
       "0  Negative  is upset that he can't update his Facebook by ...\n",
       "1  Negative  @Kenichan I dived many times for the ball. Man...\n",
       "2  Negative    my whole body feels itchy and like its on fire \n",
       "3  Negative  @nationwideclass no, it's not behaving at all....\n",
       "4  Negative                      @Kwesidei not the whole crew "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets containing user mention (@<something>): 793950\n",
      "Number of tweets containing links (http<something>): 70941\n"
     ]
    }
   ],
   "source": [
    "# First, an we need to clean the dataset. \n",
    "# Lets find how many tweets of the 1.6mil contain a user mention or a link\n",
    "mentioncounter = 0\n",
    "linkcounter = 0\n",
    "\n",
    "for tweet in bigdf['TweetText']:\n",
    "\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "        if word.startswith('@'):\n",
    "            mentioncounter += 1\n",
    "        if word.startswith('http'):\n",
    "            linkcounter += 1\n",
    "\n",
    "print(\"Number of tweets containing user mention (@<something>):\", mentioncounter)\n",
    "print(\"Number of tweets containing links (http<something>):\", linkcounter)\n",
    "\n",
    "del mentioncounter\n",
    "del linkcounter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely need to clear them first. If there was image or emoji data, we would clear that as well, but this dataset is already cleaned to a degree, and does not contain those two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK is an excellent library for simple jobs here\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# SnowballStemmer is a class of stemmers based on Porter's Snowball language for stemming, and supports multiple languages\n",
    "# it is derived from the original Porter stemmer, and features some imporvements\n",
    "# eg stemming 'Sportingly' -> 'Sport' instead of 'Sportingly' -> 'Sportingli' like Porter would\n",
    "# A stemmer reduces words to a more base form, significantly contributing to slimming our corpus down\n",
    "# It makes vectorizing the words easier, and gives more consistent results\n",
    "\n",
    "bigdf['Clean'] = bigdf['TweetText'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x)) # simple removing of special chars (non word, whitespace)\n",
    "bigdf['Clean'] = bigdf['Clean'].apply(lambda x: re.sub(r'http\\S+://', '', x)) # https -> one or more whitespace -> ://\n",
    "bigdf['Clean'] = bigdf['Clean'].apply(lambda x: str(x).lower().strip()) # Ensure we dont try to lowercase a full number tweet, followed by stripping whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we temove stopwords by crossrefering if a given word is in the stopwords list or not\n",
    "# then we stem the words down\n",
    "# remeber to tokenize first ! otherwise 'for word in x' runs for every. single. letter.\n",
    "# bear with the below, it takes about a few minutes to run\n",
    "\n",
    "stpwrds = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Tokenize\n",
    "bigdf['Clean'] = bigdf['Clean'].apply(lambda x: word_tokenize(x)) # upwards of 2 mins on my pc\n",
    "\n",
    "# Stopword removal\n",
    "bigdf['Clean'] = bigdf['Clean'].apply(lambda x: [word for word in x if word not in stpwrds]) # upwards of 30s on my pc\n",
    "\n",
    "# stemming\n",
    "bigdf['Clean'] = bigdf['Clean'].apply(lambda x: [stemmer.stem(word) for word in x]) # around 2 min on my pc\n",
    "\n",
    "# remember to rejoin! we dont want one-hot enconding or count vectorizing, but the whole string for word2vec\n",
    "bigdf['Clean'] = bigdf['Clean'].apply(lambda x: ' '.join(x)) # around a second or so? idk\n",
    "\n",
    "# next, if by any chance we have any random tweets with exact same strings remaining\n",
    "bigdf = bigdf.drop_duplicates(subset='Clean') # instant\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>TweetText</th>\n",
       "      <th>Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Negative</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>upset cant updat facebook text might cri resul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>kenichan dive mani time ball manag save 50 res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>whole bodi feel itchi like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Negative</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>nationwideclass behav im mad cant see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>kwesidei whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                          TweetText  \\\n",
       "0  Negative  is upset that he can't update his Facebook by ...   \n",
       "1  Negative  @Kenichan I dived many times for the ball. Man...   \n",
       "2  Negative    my whole body feels itchy and like its on fire    \n",
       "3  Negative  @nationwideclass no, it's not behaving at all....   \n",
       "4  Negative                      @Kwesidei not the whole crew    \n",
       "\n",
       "                                               Clean  \n",
       "0  upset cant updat facebook text might cri resul...  \n",
       "1  kenichan dive mani time ball manag save 50 res...  \n",
       "2                    whole bodi feel itchi like fire  \n",
       "3              nationwideclass behav im mad cant see  \n",
       "4                                kwesidei whole crew  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save this so we can quickly access the preprocessed data anytime\n",
    "import zipfile\n",
    "\n",
    "# Select the relevant columns\n",
    "relevant_columns = ['target', 'Clean']\n",
    "filtered_df = bigdf[relevant_columns]\n",
    "\n",
    "# Save the dataset as CSV\n",
    "filtered_df.to_csv('preprocessed_dataset.csv', index=False)\n",
    "\n",
    "# Zip the CSV file\n",
    "with zipfile.ZipFile('preprocessed_dataset.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write('preprocessed_dataset.csv')\n",
    "\n",
    "# i will be pushing both, zipped and unzipped to the repo\n",
    "# just in case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps is model training, so general outline is \n",
    "\n",
    "    train-test split\n",
    "\n",
    "    encoding\n",
    "\n",
    "    word embeddings (word2vec, text2vec-openai, etc)\n",
    "\n",
    "    model layering (make your pick from keras, pytorch, etc)\n",
    "\n",
    "    optimization \n",
    "\n",
    "    evaluation\n",
    "\n",
    "And for the project we also have a flask deployment concept"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
